# ğŸš€ è´¢æ”¿å·¥èµ„å¤„ç†ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–å»ºè®®

æœ¬æ–‡æ¡£æä¾›äº†ä¸€ç³»åˆ—é’ˆå¯¹è´¢æ”¿å·¥èµ„å¤„ç†ç³»ç»Ÿçš„æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼Œæ—¨åœ¨å‡å°‘å¤„ç†æ—¶é—´ï¼Œæé«˜å“åº”æ€§ï¼Œå¹¶æ”¹å–„ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“Š æ•°æ®å¤„ç†æ€§èƒ½ä¼˜åŒ–

### 1ï¸âƒ£ Pandas æ•°æ®å¤„ç†ä¼˜åŒ–

#### å‘é‡åŒ–æ“ä½œæ›¿ä»£å¾ªç¯

```python
# ä¼˜åŒ–å‰ï¼šä½¿ç”¨å¾ªç¯å¤„ç†æ•°æ®
for idx, row in df.iterrows():
    df.at[idx, 'å®å‘å·¥èµ„'] = row['åº”å‘å·¥èµ„'] - row['æ‰£å‘åˆè®¡']

# ä¼˜åŒ–åï¼šä½¿ç”¨å‘é‡åŒ–æ“ä½œ
df['å®å‘å·¥èµ„'] = df['åº”å‘å·¥èµ„'] - df['æ‰£å‘åˆè®¡']
```

#### å‡å°‘ DataFrame å¤åˆ¶

```python
# ä¼˜åŒ–å‰ï¼šåˆ›å»ºä¸å¿…è¦çš„å‰¯æœ¬
temp_df = df.copy()
temp_df['æ–°åˆ—'] = è®¡ç®—ç»“æœ
return temp_df

# ä¼˜åŒ–åï¼šé€‚å½“æƒ…å†µä¸‹ä½¿ç”¨ inplace å‚æ•°
df['æ–°åˆ—'] = è®¡ç®—ç»“æœ
return df
```

#### é¢„å…ˆåˆ†é…å†…å­˜

```python
# ä¼˜åŒ–å‰ï¼šåŠ¨æ€å¢é•¿åˆ—è¡¨
results = []
for item in large_list:
    results.append(process(item))

# ä¼˜åŒ–åï¼šé¢„å…ˆåˆ†é…å†…å­˜
results = [None] * len(large_list)
for i, item in enumerate(large_list):
    results[i] = process(item)
```

#### ä½¿ç”¨é€‚å½“çš„æ•°æ®ç±»å‹

```python
# æŒ‡å®šç²¾ç¡®çš„æ•°æ®ç±»å‹ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨
df = pd.read_excel(file_path, dtype={
    'å‘˜å·¥ç¼–å·': str,
    'å·¥é¾„': 'int32',
    'åŸºæœ¬å·¥èµ„': 'float32'
})
```

### 2ï¸âƒ£ Excel æ–‡ä»¶å¤„ç†ä¼˜åŒ–

#### ä»…è¯»å–å¿…è¦è¡Œåˆ—

```python
# ä¼˜åŒ–å‰ï¼šè¯»å–æ•´ä¸ªæ–‡ä»¶
df = pd.read_excel(file_path)

# ä¼˜åŒ–åï¼šåªè¯»å–éœ€è¦çš„è¡Œåˆ—
df = pd.read_excel(
    file_path,
    skiprows=header_row + 1,
    usecols=needed_columns,
    nrows=max_rows
)
```

#### åˆ†æ‰¹å¤„ç†å¤§æ–‡ä»¶

```python
def process_large_file(file_path, chunk_size=1000):
    # åˆ›å»º ExcelFile å¯¹è±¡ä»¥é¿å…å¤šæ¬¡æ‰“å¼€æ–‡ä»¶
    with pd.ExcelFile(file_path) as xls:
        # è·å–æ€»è¡Œæ•°
        total_rows = pd.read_excel(xls, nrows=0).shape[0]
        results = []
        
        # åˆ†æ‰¹è¯»å–å¤„ç†
        for i in range(0, total_rows, chunk_size):
            chunk = pd.read_excel(
                xls,
                skiprows=i,
                nrows=min(chunk_size, total_rows - i)
            )
            results.append(process_chunk(chunk))
        
        # åˆå¹¶ç»“æœ
        return pd.concat(results, ignore_index=True)
```

## ğŸ’» UI å“åº”æ€§ä¼˜åŒ–

### 1ï¸âƒ£ Streamlit ç‰¹æ€§åˆ©ç”¨

#### ä½¿ç”¨ç¼“å­˜åŠŸèƒ½

```python
@st.cache_data
def load_mapping_rules(file_content):
    """
    åŠ è½½æ˜ å°„è§„åˆ™ï¼Œç»“æœä¼šè¢«ç¼“å­˜
    """
    return json.loads(file_content)

@st.cache_resource
def get_complex_resource():
    """
    åˆå§‹åŒ–å’Œè¿”å›å¤æ‚èµ„æºï¼Œå¦‚æ¨¡å‹
    """
    return initialize_resource()
```

#### ä½¿ç”¨ä¼šè¯çŠ¶æ€é«˜æ•ˆå­˜å‚¨

```python
# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'processed_data' not in st.session_state:
    st.session_state.processed_data = None

# å¤„ç†æ•°æ®å¹¶å­˜å‚¨
if st.button('å¤„ç†æ•°æ®'):
    st.session_state.processed_data = process_data(uploaded_file)

# åœ¨å¦ä¸€ä¸ªç»„ä»¶ä¸­ä½¿ç”¨å¤„ç†åçš„æ•°æ®
if st.session_state.processed_data is not None:
    st.dataframe(st.session_state.processed_data)
```

#### ä½¿ç”¨ Streamlit è¿›åº¦æŒ‡ç¤ºå™¨

```python
def process_files(files):
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    results = []
    for i, file in enumerate(files):
        # æ›´æ–°è¿›åº¦
        progress = (i + 1) / len(files)
        progress_bar.progress(progress)
        status_text.text(f"å¤„ç†æ–‡ä»¶ {i+1}/{len(files)}: {file.name}")
        
        # å¤„ç†æ–‡ä»¶
        result = process_single_file(file)
        results.append(result)
    
    return results
```

### 2ï¸âƒ£ å¼‚æ­¥å¤„ç†

#### ä½¿ç”¨åå°çº¿ç¨‹å¤„ç†è€—æ—¶ä»»åŠ¡

```python
import threading

def start_processing():
    # æ˜¾ç¤ºå¤„ç†å¼€å§‹
    st.info("å¤„ç†å·²åœ¨åå°å¼€å§‹ï¼Œè¯·ç­‰å¾…...")
    
    # åˆ›å»ºå¹¶å¯åŠ¨åå°çº¿ç¨‹
    thread = threading.Thread(target=background_processing)
    thread.start()

def background_processing():
    # åœ¨åå°æ‰§è¡Œè€—æ—¶æ“ä½œ
    result = process_data(...)
    
    # å°†ç»“æœå­˜å‚¨åœ¨ä¼šè¯çŠ¶æ€ä¸­
    st.session_state.processing_result = result
    st.session_state.processing_done = True
```

## ğŸ§  å†…å­˜ç®¡ç†ä¼˜åŒ–

### 1ï¸âƒ£ èµ„æºé‡Šæ”¾

#### æ˜¾å¼é‡Šæ”¾å¤§å‹å¯¹è±¡

```python
def process_large_dataset():
    # åˆ›å»ºå¤§å‹æ•°æ®é›†
    large_df = create_large_dataframe()
    
    # å¤„ç†æ•°æ®
    result = process(large_df)
    
    # æ˜¾å¼åˆ é™¤ä¸å†éœ€è¦çš„å¤§å‹å¯¹è±¡
    del large_df
    import gc
    gc.collect()
    
    return result
```

#### ä½¿ç”¨ç”Ÿæˆå™¨å¤„ç†å¤§æ•°æ®

```python
def process_items_generator(items):
    """å¤„ç†é¡¹ç›®çš„ç”Ÿæˆå™¨ç‰ˆæœ¬"""
    for item in items:
        # å¤„ç†å•ä¸ªé¡¹ç›®
        result = process_item(item)
        yield result

# ä½¿ç”¨ç”Ÿæˆå™¨é€é¡¹å¤„ç†
for processed_item in process_items_generator(large_list):
    # ä½¿ç”¨å¤„ç†åçš„é¡¹ç›®
    use_result(processed_item)
```

## ğŸ“ˆ è®¡ç®—ä¼˜åŒ–

### 1ï¸âƒ£ å¤æ‚è®¡ç®—ä¼˜åŒ–

#### é¢„è®¡ç®—å’Œç¼“å­˜ä¸­é—´ç»“æœ

```python
# ä¼˜åŒ–å‰ï¼šé‡å¤è®¡ç®—
for row in df.iterrows():
    value1 = expensive_calculation1(row)
    value2 = expensive_calculation2(row)
    result = value1 + value2

# ä¼˜åŒ–åï¼šè®¡ç®—ä¸€æ¬¡å¹¶å­˜å‚¨ç»“æœ
df['temp_value1'] = df.apply(expensive_calculation1, axis=1)
df['temp_value2'] = df.apply(expensive_calculation2, axis=1)
df['result'] = df['temp_value1'] + df['temp_value2']
# å¯ä»¥é€‰æ‹©åˆ é™¤ä¸´æ—¶åˆ—
df.drop(['temp_value1', 'temp_value2'], axis=1, inplace=True)
```

#### ä½¿ç”¨ NumPy åŠ é€Ÿè®¡ç®—

```python
import numpy as np

# ä¼˜åŒ–å‰ï¼šä½¿ç”¨Pythonè¿ç®—
result = []
for a, b, c in zip(list_a, list_b, list_c):
    result.append(a * b + c)

# ä¼˜åŒ–åï¼šä½¿ç”¨NumPyå‘é‡åŒ–è¿ç®—
arr_a = np.array(list_a)
arr_b = np.array(list_b)
arr_c = np.array(list_c)
result = arr_a * arr_b + arr_c
```

## ğŸ” æ•°æ®éªŒè¯ä¼˜åŒ–

### å‡å°‘é‡å¤éªŒè¯

```python
# ä¼˜åŒ–å‰ï¼šæ¯æ¬¡éƒ½å®Œæ•´éªŒè¯
def process_row(row):
    # æ¯è¡Œéƒ½éªŒè¯å­—æ®µ
    if validate_fields(row):
        # å¤„ç†è¡Œ
        return transform(row)
    return None

# ä¼˜åŒ–åï¼šéªŒè¯ä¸€æ¬¡å­—æ®µå­˜åœ¨æ€§
def process_dataframe(df):
    # ä¸€æ¬¡æ€§éªŒè¯æ‰€æœ‰å¿…è¦å­—æ®µæ˜¯å¦å­˜åœ¨
    if not all(field in df.columns for field in required_fields):
        raise ValueError("ç¼ºå°‘å¿…è¦å­—æ®µ")
    
    # å¤„ç†æ•´ä¸ªDataFrame
    return df.apply(transform, axis=1)
```

## ğŸ“¦ Dockerä¼˜åŒ–

### 1ï¸âƒ£ é•œåƒä½“ç§¯ä¼˜åŒ–

#### ä½¿ç”¨å¤šé˜¶æ®µæ„å»º

```dockerfile
# æ„å»ºé˜¶æ®µ
FROM python:3.12-slim AS builder

WORKDIR /app
COPY requirements.txt .
RUN pip install --user -r requirements.txt

# è¿è¡Œé˜¶æ®µ
FROM python:3.12-slim

WORKDIR /app
COPY --from=builder /root/.local /root/.local
COPY . .

ENV PATH=/root/.local/bin:$PATH

CMD ["streamlit", "run", "app.py"]
```

#### ä½¿ç”¨ .dockerignore

```
# .dockerignore æ–‡ä»¶
__pycache__/
*.py[cod]
*$py.class
.git/
.idea/
.vscode/
venv/
```

## ğŸ“Œ æœ€ä½³å®è·µæ€»ç»“

1. **æ•°æ®å¤„ç†ä¼˜åŒ–**:
   - å°½å¯èƒ½ä½¿ç”¨å‘é‡åŒ–æ“ä½œ
   - é¿å…ä¸å¿…è¦çš„æ•°æ®å¤åˆ¶
   - ä½¿ç”¨é€‚å½“çš„æ•°æ®ç±»å‹
   - ä»…è¯»å–å¿…è¦çš„æ•°æ®

2. **UIå“åº”ä¼˜åŒ–**:
   - åˆç†ä½¿ç”¨Streamlitç¼“å­˜
   - æä¾›è¿›åº¦åé¦ˆ
   - å®ç°åå°å¤„ç†

3. **å†…å­˜ç®¡ç†**:
   - åŠæ—¶é‡Šæ”¾å¤§å¯¹è±¡
   - ä½¿ç”¨ç”Ÿæˆå™¨å¤„ç†å¤§æ•°æ®é›†
   - è€ƒè™‘æ•°æ®çš„ç”Ÿå‘½å‘¨æœŸ

4. **è®¡ç®—ä¼˜åŒ–**:
   - é¢„è®¡ç®—å’Œç¼“å­˜ä¸­é—´ç»“æœ
   - åˆ©ç”¨NumPyè¿›è¡Œå‘é‡åŒ–è®¡ç®—
   - é¿å…é‡å¤éªŒè¯

5. **Dockerä¼˜åŒ–**:
   - ä½¿ç”¨å¤šé˜¶æ®µæ„å»º
   - ä½¿ç”¨.dockerignoreå‡å°é•œåƒå¤§å°
   - è€ƒè™‘ä½¿ç”¨Alpineé•œåƒè¿›ä¸€æ­¥å‡å°ä½“ç§¯

---

å®æ–½è¿™äº›ä¼˜åŒ–ç­–ç•¥å¯ä»¥æ˜¾è‘—æé«˜ç³»ç»Ÿçš„æ€§èƒ½å’Œå“åº”æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§é‡æ•°æ®æ—¶ã€‚æ ¹æ®å…·ä½“æƒ…å†µï¼Œé€‰æ‹©æœ€é€‚åˆçš„ä¼˜åŒ–æ–¹æ³•ï¼Œå¹¶è®°å¾—åœ¨ä¼˜åŒ–å‰åè¿›è¡Œæ€§èƒ½æµ‹è¯•ï¼Œä»¥é‡åŒ–æ”¹è¿›æ•ˆæœã€‚ 