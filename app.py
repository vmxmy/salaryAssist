import streamlit as st
import pandas as pd
import tempfile
import os
from datetime import datetime
from fiscal_report_full_script import process_sheet, format_excel_with_styles
import json
import matplotlib.pyplot as plt
import numpy as np # ç¡®ä¿å¯¼å…¥ numpy
import re # Add import for regex
from streamlit_markdown import st_markdown # Import the component

# Helper function to sanitize text for Mermaid IDs
def sanitize_for_mermaid_id(text):
    # Remove leading/trailing whitespace
    text = str(text).strip()
    # Replace sequences of non-alphanumeric characters (excluding hyphen allowed internally) with a single underscore
    text = re.sub(r'[^\w-]+', '_', text)
    # Ensure it doesn't start with a number or underscore if possible, prepend 'n' if it does
    if text and (text[0].isdigit() or text[0] == '_'):
        text = 'n' + text
    # Handle empty string case
    if not text:
        return "empty_node"
    # Limit length to avoid overly long IDs (adjust limit as needed)
    return text[:50]

# --- Matplotlib ä¸­æ–‡æ˜¾ç¤ºè®¾ç½® ---
# ä½¿ç”¨ä»ç³»ç»Ÿä¸­æ‰¾åˆ°çš„å­—ä½“ï¼Œä¼˜å…ˆ Lantinghei SC
plt.rcParams['font.sans-serif'] = ['Lantinghei SC', 'SimSong', 'Kaiti SC', 'Songti SC', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False
# --- ç»“æŸè®¾ç½® ---

# --- åˆå§‹åŒ– Session State ---
if 'log_messages' not in st.session_state:
    st.session_state.log_messages = []
if 'mapping_data' not in st.session_state:
    st.session_state.mapping_data = None
if 'mapping_valid' not in st.session_state:
    st.session_state.mapping_valid = None # None: æœªä¸Šä¼ , True: æœ‰æ•ˆ, False: æ— æ•ˆ
# --- æ–°å¢ Session State for Single Select --- #
if 'single_selected_identity_column' not in st.session_state:
    st.session_state.single_selected_identity_column = None
# --- ç»“æŸåˆå§‹åŒ– ---

# --- æ—¥å¿—è®°å½•å‡½æ•° ---
def log(message, level="INFO"):
    now = datetime.now().strftime("%H:%M:%S")
    level_icon_map = {"INFO": "â„¹ï¸", "WARNING": "âš ï¸", "ERROR": "âŒ", "SUCCESS": "âœ…"}
    icon = level_icon_map.get(level, "â–ªï¸")
    log_level_class = f"log-{level.lower()}"

    # Wrap prefix and message in spans for styling
    formatted_message = f"<span class='log-prefix'>{now} {icon}</span> <span class='{log_level_class}'>{message}</span>"

    st.session_state.log_messages.append(formatted_message)
    # æ³¨æ„ï¼šæ—¥å¿—åœ¨ä¾§è¾¹æ çš„æ›´æ–°é€šå¸¸åœ¨æ•´ä¸ªæŒ‰é’®è„šæœ¬æ‰§è¡Œå®Œåå‘ç”Ÿ
# --- ç»“æŸæ—¥å¿—å‡½æ•° ---

st.set_page_config(layout="wide", page_title="è´¢æ”¿å·¥èµ„å¤„ç†ç³»ç»Ÿ")

# --- NEW CSS for Modern Minimalist Style ---
modern_minimalist_css = """
<style>
/* Import Google Font */
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');

/* Global Styles */
html, body, [class*="st-"] {{
    font-family: 'Inter', sans-serif;
    background-color: #F8F9FA; /* Light grey background */
    color: #212529; /* Dark text */
}}

/* Sidebar Styles */
[data-testid="stSidebar"] {{
    background-color: #FFFFFF; /* White sidebar */
    border-right: 1px solid #E0E0E0; /* Subtle border */
    box-shadow: 2px 0px 5px rgba(0, 0, 0, 0.05); /* Slight shadow */
}}

/* Titles and Headers */
h1, h2, h3, h4, h5, h6 {{
    font-family: 'Inter', sans-serif;
    font-weight: 600; /* Bolder headers */
}}

/* Custom simple subheader style (replaces old .custom-subheader) */
.simple-subheader {{
    font-size: 1.5em; /* H3 size equivalent */
    font-weight: 600;
    margin-top: 25px;
    margin-bottom: 15px;
    border-bottom: 1px solid #E0E0E0; /* Subtle underline */
    padding-bottom: 8px;
}}

/* Input Controls */
[data-testid="stTextInput"] input,
[data-testid="stDateInput"] input,
[data-testid="stSelectbox"] div[role="combobox"] {{
    border: 1px solid #ced4da;
    border-radius: 0.3rem;
    background-color: #FFFFFF;
}}

/* File Uploader Adjustments */
[data-testid="stFileUploader"] {{
    /* Add some space */
    margin-bottom: 10px;
}}

/* Primary Button Styling */
button[kind="primary"] {{
    background-color: #0d6efd; /* Primary blue */
    color: white;
    padding: 0.75rem 1.25rem; /* Moderate padding */
    font-size: 1rem; /* Standard font size */
    font-weight: 600;
    border-radius: 0.3rem; /* Consistent rounding */
    border: none; /* Remove default border */
    transition: background-color 0.2s ease-in-out; /* Hover effect */
}}

button[kind="primary"]:hover {{
    background-color: #0b5ed7; /* Darker blue on hover */
}}

/* --- æ›´æ–°ï¼šä¸ºé»˜è®¤æŒ‰é’®æ·»åŠ ç°ä»£è½®å»“æ ·å¼ --- */
button:not([kind="primary"]) {{
    background-color: transparent;
    color: #198754; /* Bootstrap success green */
    border: 1px solid #198754;
    padding: 0.75rem 1.25rem; /* Match primary button padding */
    font-size: 1rem; /* Match primary font size */
    font-weight: 600; /* Match primary font weight */
    border-radius: 0.5rem; /* Softer corners */
    transition: background-color 0.2s ease-in-out, color 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
}}

button:not([kind="primary"]):hover {{
    background-color: #198754; /* Fill with green on hover */
    color: white;
    border-color: #198754;
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.08);
}}
/* --- ç»“æŸæ›´æ–° --- */

/* Log Message Styling */
.log-info {{ color: #0dcaf0; }} /* Cyan info */
.log-warning {{ color: #ffc107; }} /* Yellow warning */
.log-error {{ color: #dc3545; font-weight: bold; }} /* Red, bold error */
.log-success {{ color: #198754; }} /* Green success */

/* Styling for the log timestamp and icon */
.log-prefix {{
    color: #6c757d; /* Grey for timestamp/icon */
    margin-right: 5px;
}}

/* Adjust log container in sidebar */
[data-testid="stSidebar"] [data-testid="stExpander"] [data-testid="stVerticalBlock"] {{
     /* Potentially adjust padding or spacing if needed */
}}

</style>
"""
# st.markdown(modern_minimalist_css, unsafe_allow_html=True) # <-- Temporarily comment out CSS

# --- æ—§ CSS (æ³¨é‡Šæ‰æˆ–åˆ é™¤) ---
# styled_header_css = """
# <style>
# .custom-subheader {
#     border-left: 5px solid #1E90FF; /* Dodger blue left border */
#     background-color: #F0F8FF; /* Alice blue background */
#     padding: 10px 15px;      /* Padding around text */
#     margin-top: 20px;         /* Space above */
#     margin-bottom: 15px;      /* Space below */
#     border-radius: 5px;       /* Rounded corners */
#     font-size: 1.75em;        /* H3 font size */
#     line-height: 1.4;         /* Adjust line height */
#     font-weight: 600;         /* Make it bolder like headers */
# }
# /* Ensure the text inside aligns well, prevent potential default margins */
# .custom-subheader h3 {
#     margin: 0;
#     padding: 0;
#     line-height: inherit; /* Inherit line height */
#     font-size: inherit; /* Inherit font size */
#     font-weight: inherit; /* Inherit font weight */
# }
#
# /* Target Streamlit's primary button to make it larger */
# button[kind="primary"] {
#     padding: 15px 30px; /* Further increase padding */
#     font-size: 2.4em;  /* Further increase font size */
#     font-weight: bold; /* Ensure font is bold */
# }
# </style>
# """
# st.markdown(styled_header_css, unsafe_allow_html=True)

# --- ä¸»ç•Œé¢ ---
st.title("å·¥èµ„è¡¨åˆå¹¶AIåŠ©æ‰‹")

# Add vertical space below the title
st.write("")
st.write("")

# --- è¡¨å•è¾“å…¥åŒºåŸŸ ---
st.sidebar.title("é«˜æ–°åŒºè´¢é‡‘å±€ç»¼åˆå¤„")
st.sidebar.header("ğŸ”§ å‚æ•°è®¾ç½®")

# è‡ªå®šä¹‰å•ä½åç§°
unit_name = st.sidebar.text_input("å•ä½åç§°", value="é«˜æ–°åŒºè´¢æ”¿å±€")

# æ—¥æœŸæ§ä»¶ï¼ˆé»˜è®¤å½“å‰æœˆä»½ï¼‰
def_year = datetime.today().year
def_month = datetime.today().month

salary_date = st.sidebar.date_input("å·¥èµ„è¡¨æ—¥æœŸï¼ˆç”¨äºæ ‡é¢˜æ ï¼‰", value=datetime(def_year, def_month, 1), format="YYYY-MM-DD")


# æ–‡ä»¶ä¸Šä¼ 
with st.expander("ğŸ“ ä¸Šä¼ æ‰€éœ€æ–‡ä»¶", expanded=True):
    col1, col2 = st.columns(2)
    with col1:
        # ä½¿ç”¨ markdown ä½œä¸ºæ ‡ç­¾ï¼Œå¹¶éšè— file_uploader è‡ªå¸¦çš„æ ‡ç­¾
        st.caption("ğŸ“ æºæ•°æ®å·¥èµ„è¡¨ï¼ˆæ”¯æŒå¤šæ–‡ä»¶ï¼‰")
        source_files = st.file_uploader("source_uploader", type=["xlsx"], accept_multiple_files=True, label_visibility="collapsed", key="source_uploader")

        st.caption("ğŸ“ å¯¼å‡ºè¡¨å­—æ®µæ¨¡æ¿")
        file_template = st.file_uploader("template_uploader", type=["xlsx"], label_visibility="collapsed", key="template_uploader")

    with col2:
        st.caption("ğŸ“ æ‰£æ¬¾é¡¹è¡¨ï¼ˆå«å§“å+å„é¡¹æ‰£æ¬¾ï¼‰")
        file_deductions = st.file_uploader("deductions_uploader", type=["xlsx"], label_visibility="collapsed", key="deductions_uploader")

        st.caption("ğŸ“ å­—æ®µæ˜ å°„è§„åˆ™ ï¼ˆJSONæ ¼å¼ï¼‰")
        file_mapping = st.file_uploader("mapping_uploader", type=["json"], label_visibility="collapsed", key="mapping_uploader")

# --- JSON æ ¡éªŒé€»è¾‘ ---
# æ¯æ¬¡è„šæœ¬è¿è¡Œæ—¶éƒ½é‡æ–°æ ¡éªŒä¸Šä¼ çš„æ–‡ä»¶çŠ¶æ€
mapping_validation_placeholder = st.empty() # ç”¨äºæ˜¾ç¤ºæ ¡éªŒç»“æœæ¶ˆæ¯
temp_mapping_data = None
if file_mapping is not None:
    try:
        # é‡ç½®æ–‡ä»¶è¯»å–æŒ‡é’ˆ
        file_mapping.seek(0)
        # å°è¯•è§£æ JSON
        temp_mapping_data = json.load(file_mapping)
        # åŸºæœ¬ç»“æ„æ£€æŸ¥ (ç¡®ä¿é¡¶å±‚æ˜¯å¯¹è±¡ï¼Œä¸”åŒ…å« 'field_mappings' åˆ—è¡¨)
        if isinstance(temp_mapping_data, dict) and isinstance(temp_mapping_data.get('field_mappings'), list):
            # åªæœ‰å½“æ–°ä¸Šä¼ çš„æ–‡ä»¶æœ‰æ•ˆæ—¶æ‰æ›´æ–° session_state
            st.session_state.mapping_data = temp_mapping_data
            st.session_state.mapping_valid = True
            mapping_validation_placeholder.success("âœ… æ˜ å°„æ–‡ä»¶ JSON æœ‰æ•ˆã€‚")
        else:
            st.session_state.mapping_valid = False # æ ‡è®°ä¸ºæ— æ•ˆï¼Œä½†ä¸æ¸…é™¤å¯èƒ½å­˜åœ¨çš„æ—§æœ‰æ•ˆæ•°æ®
            mapping_validation_placeholder.error("âŒ JSON æ–‡ä»¶é¡¶å±‚ç»“æ„é”™è¯¯ï¼šéœ€è¦åŒ…å« 'field_mappings' åˆ—è¡¨ã€‚")
            # å¯é€‰ï¼šæ¸…é™¤æ—§æ•°æ® st.session_state.mapping_data = None

    except json.JSONDecodeError as e:
        st.session_state.mapping_valid = False
        mapping_validation_placeholder.error(f"âŒ æ˜ å°„æ–‡ä»¶ JSON è¯­æ³•é”™è¯¯ï¼š\nåœ¨è¡Œ {e.lineno} åˆ— {e.colno} é™„è¿‘: {e.msg}")
        # å¯é€‰ï¼šæ¸…é™¤æ—§æ•°æ® st.session_state.mapping_data = None
    except Exception as e: # æ•è·å…¶ä»–å¯èƒ½çš„è¯»å–é”™è¯¯
        st.session_state.mapping_valid = False
        mapping_validation_placeholder.error(f"âŒ è¯»å–æˆ–è§£ææ˜ å°„æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        # å¯é€‰ï¼šæ¸…é™¤æ—§æ•°æ® st.session_state.mapping_data = None
else:
    # å¦‚æœ file_mapping æ§ä»¶ä¸­æ²¡æœ‰æ–‡ä»¶ï¼ˆä¾‹å¦‚ç”¨æˆ·æ¸…é™¤äº†ä¸Šä¼ ï¼‰ï¼Œä¹Ÿåº”æ›´æ–°çŠ¶æ€
    # å¦‚æœå¸Œæœ›ä¿ç•™ä¸Šæ¬¡æœ‰æ•ˆä¸Šä¼ çš„æ•°æ®ï¼Œå¯ä»¥æ³¨é‡Šæ‰ä¸‹é¢ä¸¤è¡Œ
    # st.session_state.mapping_data = None
    st.session_state.mapping_valid = None # è®¾ä¸º None è¡¨ç¤ºæœªä¸Šä¼ çŠ¶æ€
# --- ç»“æŸ JSON æ ¡éªŒ ---

# --- æ–°å¢ï¼šç‹¬ç«‹çŠ¶æ€æ¡ --- #
st.sidebar.subheader("ğŸ“Š æ–‡ä»¶ä¸Šä¼ çŠ¶æ€")

required_files_status = {
    "æºæ•°æ®å·¥èµ„è¡¨": bool(source_files),
    "æ‰£æ¬¾é¡¹è¡¨": bool(file_deductions),
    "å­—æ®µæ˜ å°„è§„åˆ™": bool(file_mapping and st.session_state.get('mapping_valid') is True)
}
optional_files_status = {
    "å¯¼å‡ºè¡¨å­—æ®µæ¨¡æ¿": bool(file_template)
}

# --- æ–°å¢ï¼šç‹¬ç«‹çŠ¶æ€æ¡ --- #
st.sidebar.markdown("**å¿…éœ€æ–‡ä»¶:**")
for file_name, is_uploaded in required_files_status.items():
    if is_uploaded:
        st.sidebar.success(f"âœ… {file_name}")
    else:
        st.sidebar.warning(f"ğŸŸ¡ {file_name} (æœªä¸Šä¼ )")

st.sidebar.markdown("**å¯é€‰æ–‡ä»¶:**")
for file_name, is_uploaded in optional_files_status.items():
    if is_uploaded:
        st.sidebar.success(f"âœ… {file_name} (å¯é€‰)")
    else:
        st.sidebar.info(f"âšªï¸ {file_name} (å¯é€‰, æœªä¸Šä¼ )")
# --- ç»“æŸæ–°å¢ --- #

st.sidebar.markdown("---")

# --- æ—¥å¿—æ˜¾ç¤ºåŒºåŸŸ ---
with st.sidebar.expander("ğŸ“„ å¤„ç†æ—¥å¿—", expanded=True):
    log_container = st.container(height=300) # å›ºå®šé«˜åº¦å¯æ»šåŠ¨å®¹å™¨
    with log_container:
        for message in st.session_state.log_messages:
            st.markdown(message, unsafe_allow_html=True) # Markdown is now expected to contain spans like <span class='log-info'>...</span>
# --- ç»“æŸæ—¥å¿—æ˜¾ç¤º ---

# ä» session_state è·å–æ•°æ®ï¼Œå¦‚æœæ— æ•ˆæˆ–ä¸º None åˆ™ç”¨ç©ºå­—å…¸
mapping_data_from_state = st.session_state.get('mapping_data') or {}
field_mappings = mapping_data_from_state.get('field_mappings', [])

# --- ç¡®ä¿ template_fields å’Œ sample_source_fields åœ¨æ­¤å¤„è¢«æ— æ¡ä»¶åˆå§‹åŒ– ---
template_fields = []
sample_source_fields = set()
# --- ç»“æŸæ— æ¡ä»¶åˆå§‹åŒ– ---

# æ¨¡æ¿å­—æ®µåé¢„å–
if file_template:
    try:
        file_template.seek(0)
        template_df_header = pd.read_excel(file_template, skiprows=2, nrows=1)
        template_fields = template_df_header.columns.tolist()
    except Exception as e:
        st.warning(f"æ¨¡æ¿å­—æ®µè¯»å–å¤±è´¥ï¼š{e}")

# æºæ•°æ®å­—æ®µç¤ºä¾‹æ”¶é›†
if source_files:
    try:
        uploaded_source_file = source_files[0]
        uploaded_source_file.seek(0)
        source_preview = pd.read_excel(uploaded_source_file, nrows=10, header=None)
        # ä¿®å¤æ‹¬å·å’Œé€»è¾‘ï¼šæŸ¥æ‰¾åŒ…å«'å§“å'æˆ–'äººå‘˜å§“å'çš„è¡Œ
        # --- ä¿®æ”¹ï¼šä½¿ç”¨é»˜è®¤å…³é”®å­—è¿›è¡Œé¦–æ¬¡æ£€æµ‹ä»¥å¡«å……é€‰é¡¹ --- #
        default_keywords_for_options = ["å§“å", "äººå‘˜å§“å"]
        header_row = next((i for i, row in source_preview.iterrows() if any((key_col in str(cell)) for cell in row.astype(str) for key_col in default_keywords_for_options)), None)
        # --- ç»“æŸä¿®æ”¹ --- #
        if header_row is not None:
            # é‡ç½®æŒ‡é’ˆï¼Œè¯»å–æ­£ç¡®çš„è¡¨å¤´è¡Œ
            uploaded_source_file.seek(0)
            df_source_cols = pd.read_excel(uploaded_source_file, skiprows=header_row, nrows=0).columns.tolist()
            sample_source_fields = set(df_source_cols)
        else:
            st.warning("æ— æ³•åœ¨ç¬¬ä¸€ä¸ªæºæ–‡ä»¶ä¸­è‡ªåŠ¨æ£€æµ‹è¡¨å¤´è¡Œä»¥è·å–ç¤ºä¾‹å­—æ®µã€‚")
    except Exception as e:
        st.warning(f"è¯»å–æºæ•°æ®ç¤ºä¾‹å­—æ®µæ—¶å‡ºé”™: {e}")

# --- æ–°å¢ï¼šå…³é”®æ ‡è¯†åˆ—é€‰æ‹© ---
with st.expander("ğŸ”‘ å…³é”®åˆ—ä¸è§„åˆ™åŒ¹é…è®¾ç½®", expanded=True):
    pre_selected_keys = [col for col in ["å§“å", "äººå‘˜å§“å"] if col in sample_source_fields]
    key_identifier_columns = st.multiselect(
        "ç”¨äºåˆå¹¶ã€æºæ•°æ®è¡¨ã€‘å’Œã€æ‰£æ¬¾è¡¨ã€‘çš„åˆ—åï¼ˆé»˜è®¤ï¼šäººå‘˜å§“åï¼‰",
        options=sample_source_fields,
        default=pre_selected_keys,
        help="è¿™äº›åˆ—å°†ç”¨äºè‡ªåŠ¨æ£€æµ‹æºæ–‡ä»¶è¡¨å¤´ï¼Œå¹¶ä½œä¸ºåˆå¹¶æ‰£æ¬¾è¡¨æ—¶çš„ä¾æ®ã€‚è¯·è‡³å°‘é€‰æ‹©ä¸€é¡¹ã€‚"
    )
    # --- ç»“æŸæ–°å¢ --- #

    # --- è§„åˆ™åŒ¹é…è®¾ç½® --- #
    # åªä¿ç•™ä¸€ä¸ªé€‰æ‹©æ¡†ï¼Œå› ä¸ºæºæ–‡ä»¶å’Œè§„åˆ™ä½¿ç”¨ç›¸åŒå­—æ®µå
    # ä½¿ç”¨ sample_source_fields (ç¡®ä¿å®ƒå·²å®šä¹‰å¹¶å¯èƒ½æ˜¯ set)
    source_cols_list = sorted(list(sample_source_fields)) if sample_source_fields else []

    # --- æ›¿æ¢ä¸º Multiselect å¹¶æ·»åŠ å¼ºåˆ¶å•é€‰é€»è¾‘ --- #
    # å°è¯•é¢„è®¾é»˜è®¤å€¼ (å¦‚æœä¹‹å‰æœ‰é€‰è¿‡)
    default_identity = None
    if st.session_state.single_selected_identity_column and st.session_state.single_selected_identity_column in source_cols_list:
        default_identity = st.session_state.single_selected_identity_column
    elif not st.session_state.single_selected_identity_column:
        # --- ä¿®æ”¹ï¼šä»…å½“ source_cols_list éç©ºæ—¶å°è¯•è®¾ç½®é»˜è®¤å€¼ --- #
        if source_cols_list:
            # å°è¯•æ™ºèƒ½é»˜è®¤ï¼šäººå‘˜èº«ä»½ > å²—ä½ç±»åˆ« > ç¬¬ä¸€ä¸ªé€‰é¡¹
            if "äººå‘˜èº«ä»½" in source_cols_list:
                default_identity = "äººå‘˜èº«ä»½"
            elif "å²—ä½ç±»åˆ«" in source_cols_list:
                default_identity = "å²—ä½ç±»åˆ«"
            elif source_cols_list: # è¿™ä¸€å±‚ elif å…¶å®å¯ä»¥çœç•¥ï¼Œå› ä¸ºå¤–å±‚ if å·²ä¿è¯éç©º
                default_identity = source_cols_list[0]
            # å°†åˆå§‹é»˜è®¤å€¼å­˜å…¥ session_state
            st.session_state.single_selected_identity_column = default_identity
        # å¦‚æœ source_cols_list ä¸ºç©ºï¼Œåˆ™ default_identity ä¿æŒ None, session state ä¹Ÿä¸º None
        # --- ç»“æŸä¿®æ”¹ --- #

    # --- ä¿®æ”¹ï¼šç¡®ä¿ default å€¼åœ¨ options ä¸­ --- #
    current_selection_in_state = []
    if st.session_state.single_selected_identity_column and st.session_state.single_selected_identity_column in source_cols_list:
        current_selection_in_state = [st.session_state.single_selected_identity_column]
    # --- ç»“æŸä¿®æ”¹ --- #

    selected_identity_list = st.multiselect(
        "ç”¨äºé€‰æ‹©å­—æ®µè½¬æ¢è§„åˆ™çš„åˆ—å (å•é€‰)", # ä¿®æ”¹æ ‡ç­¾ä»¥æç¤ºå•é€‰
        options=source_cols_list,
        default=current_selection_in_state,
        key="identity_multiselect", # æ·»åŠ  key
        help="é€‰æ‹©æºæ–‡ä»¶å’ŒJSONè§„åˆ™ä¸­éƒ½ä½¿ç”¨çš„é‚£ä¸ªå­—æ®µåè¿›è¡ŒåŒ¹é…ï¼ˆå¦‚ äººå‘˜èº«ä»½, å²—ä½ç±»åˆ«ï¼‰ã€‚è™½ä¸ºå¤šé€‰æ¡†ï¼Œä½†ä»…ç¬¬ä¸€ä¸ªé€‰é¡¹ç”Ÿæ•ˆã€‚"
    )

    # å¼ºåˆ¶å•é€‰é€»è¾‘
    if len(selected_identity_list) == 0:
        if st.session_state.single_selected_identity_column is not None:
            st.session_state.single_selected_identity_column = None
            st.rerun()
    elif len(selected_identity_list) == 1:
        if st.session_state.single_selected_identity_column != selected_identity_list[0]:
            st.session_state.single_selected_identity_column = selected_identity_list[0]
            st.rerun()
    elif len(selected_identity_list) > 1:
        # å¦‚æœç”¨æˆ·é€‰äº†å¤šä¸ªï¼Œåªä¿ç•™æœ€åä¸€ä¸ªï¼ˆæœ€æ–°çš„é€‰æ‹©ï¼‰
        last_selected = selected_identity_list[-1]
        if st.session_state.single_selected_identity_column != last_selected:
             st.session_state.single_selected_identity_column = last_selected
             st.rerun()
        # å¦åˆ™ï¼Œå¦‚æœæœ€åä¸€ä¸ªå’Œ state ç›¸åŒï¼Œä½†ä»æœ‰å¤šé€‰ï¼Œä¹Ÿéœ€è¦å¼ºåˆ¶åˆ·æ–°å›å•é€‰
        elif len(current_selection_in_state) != 1 or current_selection_in_state[0] != last_selected:
             st.rerun()
    # --- ç»“æŸæ›¿æ¢å’Œé€»è¾‘ --- #

    # --- ç»“æŸè§„åˆ™åŒ¹é…è®¾ç½® --- #

# åªæœ‰å½“æ˜ å°„æ–‡ä»¶æœ‰æ•ˆæ—¶æ‰æ˜¾ç¤ºç¼–è¾‘å’Œå¯è§†åŒ–åŒºåŸŸ
if st.session_state.get('mapping_valid') is True and 'mapping_data' in st.session_state and st.session_state.mapping_data:
    with st.expander("ğŸ“Š æ˜ å°„è§„åˆ™å¯è§†åŒ–", expanded=True): # Default to expanded
        field_mappings = st.session_state.mapping_data.get('field_mappings', [])
        identity_key = st.session_state.get('single_selected_identity_column')

        if not field_mappings:
            st.info("æ²¡æœ‰åŠ è½½æœ‰æ•ˆçš„æ˜ å°„è§„åˆ™ã€‚")
        elif not identity_key:
            st.warning("è¯·å…ˆåœ¨ä¸Šæ–¹é€‰æ‹©ç”¨äºæ ‡è¯†è§„åˆ™çš„åˆ—åã€‚")
        else:
            # Extract unique rule identifiers and sort them
            try:
                 rule_identities = sorted(list(set(str(rule.get(identity_key, "æœªçŸ¥è§„åˆ™")) for rule in field_mappings if identity_key in rule)))
            except Exception as e:
                 st.error(f"æå–è§„åˆ™æ ‡è¯†ç¬¦æ—¶å‡ºé”™: {e}")
                 rule_identities = []

            if not rule_identities:
                 st.warning(f"æ— æ³•ä»æ˜ å°„æ•°æ®ä¸­æ‰¾åˆ°åŸºäº '{identity_key}' çš„æœ‰æ•ˆè§„åˆ™æ ‡è¯†ã€‚")
            else:
                # Create a mapping from identity value to establishment value for display
                identity_to_bianzhi = {}
                for rule in field_mappings:
                    identity_val = str(rule.get(identity_key))
                    if identity_val not in identity_to_bianzhi: # Store the first encountered 'ç¼–åˆ¶' for each identity
                        identity_to_bianzhi[identity_val] = rule.get('ç¼–åˆ¶', 'æœªçŸ¥')

                # Define a function to format the display options
                format_func = lambda identity: f"{identity} (ç¼–åˆ¶: {identity_to_bianzhi.get(identity, 'æœªçŸ¥')})"

                # Options are just the rule identities
                options = rule_identities
                # Default index is 0 (first item) as rule_identities is not empty here
                default_index = 0

                selected_identity = st.selectbox(
                    f"é€‰æ‹©è¦æŸ¥çœ‹è§„åˆ™çš„ **{identity_key}**:",
                    options=options,
                    index=default_index,
                    format_func=format_func # Use the format function for display
                )

                # Find the selected rule
                selected_rule = next((rule for rule in field_mappings if str(rule.get(identity_key)) == selected_identity), None)

                if selected_rule:
                    # --- Generate Mermaid String ---
                    mermaid_lines = ["graph LR;"]
                    # Optional: Add classDefs for styling
                    mermaid_lines.append("    classDef sourceNode fill:#e0f2fe,stroke:#3b82f6,stroke-width:2px,color:#333;")
                    mermaid_lines.append("    classDef targetNode fill:#dcfce7,stroke:#16a34a,stroke-width:2px,color:#333;")
                    mermaid_lines.append("    classDef calcNode fill:#fef9c3,stroke:#f59e0b,stroke-width:2px,color:#333;")

                    nodes_defined = set()
                    rule_identity_safe = sanitize_for_mermaid_id(selected_identity)

                    for j, mapping in enumerate(selected_rule.get("mappings", [])):
                        target_field = mapping.get("target_field", f"æœªçŸ¥ç›®æ ‡_{j}")
                        target_id_base = sanitize_for_mermaid_id(target_field)
                        # Ensure unique ID even if target names repeat in a rule
                        target_id = f"tgt_{rule_identity_safe}_{target_id_base}_{j}"

                        if "source_field" in mapping:
                            source_field = mapping.get("source_field", f"æœªçŸ¥æº_{j}")
                            source_id_base = sanitize_for_mermaid_id(source_field)
                            # Ensure unique source node ID within this graph
                            source_id = f"src_{rule_identity_safe}_{source_id_base}_{j}"

                            if source_id not in nodes_defined:
                                 mermaid_lines.append(f'    {source_id}["{source_field}"]:::sourceNode')
                                 nodes_defined.add(source_id)
                            if target_id not in nodes_defined:
                                 mermaid_lines.append(f'    {target_id}["{target_field}"]:::targetNode')
                                 nodes_defined.add(target_id)
                            mermaid_lines.append(f"    {source_id} --> {target_id};")

                        elif "source_fields" in mapping:
                            source_fields = mapping.get("source_fields", [])
                            calculation = mapping.get("calculation", "æœªçŸ¥è®¡ç®—")
                            # Simplify label content, ensure newline works, remove quote replacement
                            target_label_content = f"{target_field}\\n(è®¡ç®—: {calculation})"
                            # Use standard NodeID["Label"] format

                            if target_id not in nodes_defined:
                                # Ensure label is quoted directly in the definition
                                mermaid_lines.append(f'    {target_id}["{target_label_content}"]:::calcNode')
                                nodes_defined.add(target_id)

                            for k, src_field in enumerate(source_fields):
                                src_id_base = sanitize_for_mermaid_id(src_field)
                                # Ensure unique source node ID within this complex mapping source list
                                src_id = f"src_{rule_identity_safe}_{src_id_base}_{j}_{k}"
                                if src_id not in nodes_defined:
                                    # Ensure label is quoted
                                    mermaid_lines.append(f'    {src_id}["{src_field}"]:::sourceNode')
                                    nodes_defined.add(src_id)
                                mermaid_lines.append(f"    {src_id} --> {target_id};")
                        else:
                            # Handle unrecognized mapping format if necessary
                            unknown_id = f"unknown_{rule_identity_safe}_{j}"
                            if unknown_id not in nodes_defined:
                                 # Ensure label is quoted
                                 mermaid_lines.append(f'    {unknown_id}["æœªçŸ¥æ˜ å°„æ ¼å¼: {str(mapping)[:30]}..."]:::error') # Optional error styling
                                 nodes_defined.add(unknown_id)

                    mermaid_string = "\n".join(mermaid_lines)

                    # Correct indentation for try/except block
                    try:
                        # Construct the full markdown string first
                        markdown_content = f"```mermaid\n{mermaid_string}\n```"
                        # --- Use st_markdown and remove container --- #
                        st_markdown(markdown_content)
                    except Exception as e:
                        st.error(f"æ¸²æŸ“ Mermaid å›¾è¡¨æ—¶å‡ºé”™: {e}")
                        st.text("ç”Ÿæˆçš„ Mermaid ä»£ç :")
                        st.code(mermaid_string, language="mermaid") # Keep debug code on error
                else:
                    st.warning(f"æœªæ‰¾åˆ°æ ‡è¯†ä¸º '{selected_identity}' çš„è§„åˆ™æ•°æ®ã€‚")
# --- Remove all previous content of the expander (validation, editing, download, save) ---

# --- ç§»åŠ¨ï¼šæ•°æ®æœ‰æ•ˆæ€§æ ¡éªŒåŒºåŸŸ --- #
with st.expander("âœ… æ•°æ®æ ¡éªŒä¸å¤„ç†", expanded=True):
    if st.button("ğŸ” æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§", key="validate_data_btn"):
        validation_placeholder = st.container() # åˆ›å»ºå®¹å™¨æ˜¾ç¤ºç»“æœ
        with validation_placeholder:

            # --- å¼€å§‹å®ç°æ··åˆæ ¡éªŒé€»è¾‘ --- #
            validation_errors = []
            validation_warnings = []

            # B. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸Šä¼  (å¤ç”¨ä¾§è¾¹æ é€»è¾‘ç¨å¾®ä¿®æ”¹)
            if not all(required_files_status.values()):
                validation_errors.append(f"å¿…éœ€æ–‡ä»¶ç¼ºå¤±æˆ–æ— æ•ˆ: {', '.join(name for name, uploaded in required_files_status.items() if not uploaded)}")
            # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦é€‰æ‹©äº†å…³é”®åˆ—å’Œèº«ä»½åˆ—
            if not key_identifier_columns:
                 validation_errors.append("è¯·åœ¨ä¸Šæ–¹'å…³é”®æ ‡è¯†åˆ—'æ§ä»¶ä¸­é€‰æ‹©è‡³å°‘ä¸€é¡¹ã€‚") # ä¿®æ”¹æç¤º
            if not selected_identity_list:
                 validation_errors.append("è¯·åœ¨ä¸Šæ–¹'ç”¨äºåŒ¹é…è§„åˆ™çš„å­—æ®µå'æ§ä»¶ä¸­é€‰æ‹©ä¸€é¡¹ã€‚") # ä¿®æ”¹æç¤º

            # å¦‚æœåŸºç¡€æ–‡ä»¶æˆ–é€‰æ‹©ç¼ºå¤±ï¼Œåˆ™ä¸ç»§ç»­æ£€æŸ¥
            if validation_errors:
                 # H. æ˜¾ç¤ºæœ€ç»ˆæ ¡éªŒç»“æœ (ä»…é”™è¯¯éƒ¨åˆ†)
                 st.error("**æ ¡éªŒå¤±è´¥:**\n" + "\n".join(validation_errors))
                 if 'validation_passed' in st.session_state: del st.session_state.validation_passed # æ¸…é™¤æ—§çŠ¶æ€

            # Start the try block for reading data
            try:
                # C. è¯»å–æ‰€æœ‰è¡¨å¤´ä¿¡æ¯
                # æ‰£æ¬¾è¡¨
                actual_deduction_fields = set()
                try:
                    file_deductions.seek(0)
                    deduction_df_headers = pd.read_excel(file_deductions, header=2, nrows=0) # åªè¯»è¡¨å¤´
                    actual_deduction_fields = set(deduction_df_headers.columns)
                except Exception as e:
                    validation_errors.append(f"è¯»å–æ‰£æ¬¾è¡¨è¡¨å¤´å¤±è´¥: {e}")

                # æ¨¡æ¿è¡¨ (å¦‚æœä¸Šä¼ )
                actual_template_fields = []
                template_available = False
                if file_template:
                    try:
                        file_template.seek(0)
                        template_df_header = pd.read_excel(file_template, skiprows=2, nrows=1)
                        actual_template_fields = template_df_header.columns.tolist()
                        template_available = True
                    except Exception as e:
                        validation_warnings.append(f"è¯»å–æ¨¡æ¿è¡¨è¡¨å¤´å¤±è´¥: {e} (ç›®æ ‡å­—æ®µæœ‰æ•ˆæ€§å°†æ— æ³•æ£€æŸ¥)")

                # æ‰€æœ‰æºæ–‡ä»¶
                all_actual_source_fields = set()
                source_read_errors = []
                default_keywords_for_header = ["å§“å", "äººå‘˜å§“å"]
                for i, src_file in enumerate(source_files):
                    try:
                        src_file.seek(0)
                        preview_df = pd.read_excel(src_file, header=None, nrows=20)
                        header_row_idx = next((idx for idx, row in preview_df.iterrows() if any((key in str(cell)) for cell in row.astype(str) for key in default_keywords_for_header)), None)
                        if header_row_idx is not None:
                             src_file.seek(0) # éœ€è¦é‡ç½®æŒ‡é’ˆä»¥æ­£ç¡®è¯»å–åˆ—
                             df_cols = pd.read_excel(src_file, header=header_row_idx, nrows=0).columns.tolist()
                             all_actual_source_fields.update(df_cols)
                        else:
                             source_read_errors.append(f"æ–‡ä»¶ '{src_file.name}' æœªèƒ½è‡ªåŠ¨æ£€æµ‹åˆ°è¡¨å¤´è¡Œ (ä½¿ç”¨é»˜è®¤å…³é”®å­—)ã€‚")
                    except Exception as e:
                         source_read_errors.append(f"è¯»å–æºæ–‡ä»¶ '{src_file.name}' çš„åˆ—åå¤±è´¥: {e}")
                if source_read_errors:
                     validation_warnings.extend(source_read_errors)
                if not all_actual_source_fields:
                     validation_errors.append("æœªèƒ½ä»ä»»ä½•æºæ–‡ä»¶ä¸­æˆåŠŸè¯»å–åˆ—åã€‚")

                # D. è¯»å– JSON è§„åˆ™ (å·²åœ¨å‰é¢åŠ è½½åˆ° st.session_state.mapping_data)
                if 'mapping_data' not in st.session_state or not st.session_state.mapping_data:
                     validation_errors.append("æ— æ³•åŠ è½½ JSON æ˜ å°„è§„åˆ™ã€‚")
                else:
                     field_mappings = st.session_state.mapping_data.get('field_mappings', [])

                     # å¦‚æœå‰é¢æ­¥éª¤æœ‰é”™è¯¯ï¼Œåˆ™åœæ­¢è¿›ä¸€æ­¥æ£€æŸ¥
                     if not validation_errors:
                         # F. æ‰§è¡Œå­—æ®µé‡å¤æ£€æŸ¥
                         common_fields = all_actual_source_fields.intersection(actual_deduction_fields)
                         repeated_non_key_fields = common_fields - set(key_identifier_columns)
                         if repeated_non_key_fields:
                             validation_errors.append(f"å­—æ®µå†²çªï¼šä»¥ä¸‹å­—æ®µåŒæ—¶å­˜åœ¨äºæºæ–‡ä»¶å’Œæ‰£æ¬¾è¡¨ä¸­ï¼ˆéå…³é”®åˆ—ï¼‰: {sorted(list(repeated_non_key_fields))}ã€‚è¯·ä¿®æ”¹åˆ—åç¡®ä¿å”¯ä¸€æ€§ã€‚")

                         # G. æ‰§è¡Œ JSON è§„åˆ™æœ‰æ•ˆæ€§æ£€æŸ¥
                         available_fields = all_actual_source_fields.union(actual_deduction_fields)
                         invalid_source_map = []
                         invalid_target_map = []

                         # --- æ–°å¢ï¼šé¢„æ”¶é›†æ‰€æœ‰å®šä¹‰çš„ç›®æ ‡å­—æ®µ --- #
                         all_defined_target_fields = set()
                         for r in field_mappings:
                             for m in r.get("mappings", []):
                                 if m.get("target_field"):
                                     all_defined_target_fields.add(m["target_field"])
                         # --- ç»“æŸæ–°å¢ --- #

                         for rule_idx, rule in enumerate(field_mappings):
                             # --- ä½¿ç”¨ Session State --- #
                             rule_id = rule.get(st.session_state.single_selected_identity_column, f"è§„åˆ™ #{rule_idx}")
                             # --- ç»“æŸä½¿ç”¨ --- #
                             for map_idx, mapping in enumerate(rule.get("mappings", [])):
                                 if "source_field" in mapping:
                                     src = mapping["source_field"]
                                     tgt = mapping.get("target_field")
                                     if src not in available_fields:
                                         invalid_source_map.append(f"è§„åˆ™ '{rule_id}': æºå­—æ®µ '{src}' åœ¨æºæ–‡ä»¶æˆ–æ‰£æ¬¾è¡¨ä¸­æœªæ‰¾åˆ°ã€‚")
                                     if template_available and tgt and tgt not in actual_template_fields:
                                         invalid_target_map.append(f"è§„åˆ™ '{rule_id}': ç›®æ ‡å­—æ®µ '{tgt}' (æ¥è‡ªæº '{src}') åœ¨æ¨¡æ¿æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ã€‚")
                                 elif "source_fields" in mapping:
                                     src_list = mapping["source_fields"]
                                     tgt = mapping.get("target_field")
                                     for src in src_list:
                                         # --- ä¿®æ”¹æ ¡éªŒæ¡ä»¶å’Œè­¦å‘Šæ¶ˆæ¯ --- #
                                         if src not in available_fields and src not in all_defined_target_fields:
                                             invalid_source_map.append(f"è§„åˆ™ '{rule_id}' (è®¡ç®—): æºå­—æ®µ '{src}' åœ¨æºæ–‡ä»¶/æ‰£æ¬¾è¡¨ä¸­æœªæ‰¾åˆ°ï¼Œä¸”æœªè¢«å…¶ä»–è§„åˆ™å®šä¹‰ä¸ºç›®æ ‡å­—æ®µã€‚")
                                         # --- ç»“æŸä¿®æ”¹ --- #
                                     if template_available and tgt and tgt not in actual_template_fields:
                                         invalid_target_map.append(f"è§„åˆ™ '{rule_id}' (è®¡ç®—): ç›®æ ‡å­—æ®µ '{tgt}' åœ¨æ¨¡æ¿æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ã€‚")

                         if invalid_source_map:
                              # --- ä¿®æ”¹è­¦å‘Šæ¶ˆæ¯æ ‡é¢˜å’Œæ ¼å¼ (ä½¿ç”¨ f-string) --- #
                              warning_list_md = "\n* ".join(invalid_source_map)
                              validation_warnings.append(f"**JSON è§„åˆ™è­¦å‘Šï¼šéƒ¨åˆ†è®¡ç®—æ‰€éœ€çš„æºå­—æ®µæ— æ³•ç›´æ¥ä»æ–‡ä»¶æˆ–ä»å…¶ä»–è§„åˆ™ç”Ÿæˆ (è¯·æ£€æŸ¥ JSON æˆ–æ–‡ä»¶):**\n* {warning_list_md}")
                              # --- ç»“æŸä¿®æ”¹ --- #
                         if invalid_target_map:
                              # --- ä¿®æ”¹é”™è¯¯æ¶ˆæ¯æ ‡é¢˜å’Œæ ¼å¼ (ä½¿ç”¨ f-string) --- #
                              error_list_md = "\n* ".join(invalid_target_map)
                              validation_errors.append(f"**JSON è§„åˆ™é”™è¯¯ï¼šéƒ¨åˆ†ç›®æ ‡å­—æ®µåœ¨æ¨¡æ¿æ–‡ä»¶ä¸­æœªæ‰¾åˆ°:**\n* {error_list_md}")
                              # --- ç»“æŸä¿®æ”¹ --- #

            except Exception as validation_ex:
                 validation_errors.append(f"æ ¡éªŒè¿‡ç¨‹ä¸­å‘ç”Ÿæ„å¤–é”™è¯¯: {validation_ex}")

            # H. æ˜¾ç¤ºæœ€ç»ˆæ ¡éªŒç»“æœ (å®Œæ•´ç‰ˆ) - Correcting indentation for this block
            if validation_errors:
                 st.error("**æ ¡éªŒå¤±è´¥:**\n" + "\n".join(validation_errors))
                 st.session_state.validation_passed = False # å¯é€‰ï¼šç”¨äºåç»­æ§åˆ¶
            elif validation_warnings: # If no errors but warnings exist
                 st.warning("**æ ¡éªŒè­¦å‘Š:**\n" + "\n".join(validation_warnings))
                 st.success("âœ… æ•°æ®å’Œè§„åˆ™æœ‰æ•ˆæ€§æ£€æŸ¥é€šè¿‡ (ä½†å­˜åœ¨è­¦å‘Š)ã€‚")
                 st.session_state.validation_passed = True
            else: # No errors, no warnings
                 st.success("âœ… æ•°æ®å’Œè§„åˆ™æœ‰æ•ˆæ€§æ£€æŸ¥é€šè¿‡ï¼")
                 st.session_state.validation_passed = True
            # Ensure failure state is set if errors occurred (redundant but safe)
            if validation_errors and ('validation_passed' not in st.session_state or st.session_state.validation_passed is not False):
                st.session_state.validation_passed = False

            # --- ç»“æŸå®ç°æ··åˆæ ¡éªŒé€»è¾‘ --- #

        validation_placeholder = st.container()
    # --- ç»“æŸæ ¡éªŒåŒºåŸŸ --- #



    # --- å¤„ç†è§¦å‘åŒºåŸŸ ---

    # --- æ–°å¢ï¼šæ ¹æ®æ ¡éªŒçŠ¶æ€å†³å®šæŒ‰é’®æ˜¯å¦å¯ç”¨ --- #
    validation_status = st.session_state.get('validation_passed', None) # None: æœªæ ¡éªŒ, False: å¤±è´¥, True: æˆåŠŸ
    disable_processing_button = (validation_status is not True)
    button_tooltip = "è¯·å…ˆç‚¹å‡»ä¸Šæ–¹çš„ 'æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§' æŒ‰é’®å¹¶é€šè¿‡æ ¡éªŒã€‚" if disable_processing_button else "å¼€å§‹åˆå¹¶å¤„ç†æ‰€æœ‰ä¸Šä¼ çš„æ–‡ä»¶ã€‚"
    # --- ç»“æŸ --- #

    if st.button("ğŸš€ å¼€å§‹å¤„ç†æ•°æ®", type="primary", disabled=disable_processing_button, help=button_tooltip):
        # æ¸…ç©ºæ—§æ—¥å¿—å¹¶è®°å½•å¼€å§‹
        st.session_state.log_messages = []
        log("å¼€å§‹å¤„ç†æµç¨‹...", "INFO")

        # 1. è¾“å…¥æ ¡éªŒ
        valid_inputs = True
        if not source_files:
            log("è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªæºæ•°æ®å·¥èµ„è¡¨ï¼", "ERROR")
            valid_inputs = False
        if not file_deductions:
            log("è¯·ä¸Šä¼ æ‰£æ¬¾é¡¹è¡¨ï¼", "ERROR")
            valid_inputs = False
        if not st.session_state.get('mapping_valid', False):
            log("å­—æ®µæ˜ å°„æ–‡ä»¶æ— æ•ˆæˆ–æœªä¸Šä¼ ï¼", "ERROR")
            valid_inputs = False
        if not selected_identity_list:
            log("è¯·é€‰æ‹©ç”¨äºåŒ¹é…è§„åˆ™çš„å­—æ®µåï¼", "ERROR")
            valid_inputs = False
        if not key_identifier_columns:
            log("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªå…³é”®æ ‡è¯†åˆ—åï¼", "ERROR")
            valid_inputs = False

        if valid_inputs:
            # 2. å‡†å¤‡æ•°æ®
            deduction_df = None
            current_field_mappings = st.session_state.get('mapping_data', {}).get('field_mappings', [])
            selected_deduction_fields = [] # åˆå§‹åŒ–

            try:
                log("è¯»å–æ‰£æ¬¾æ•°æ®...", "INFO")
                with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp_deduct:
                    file_deductions.seek(0)
                    tmp_deduct.write(file_deductions.getvalue())
                    tmp_deduct_path = tmp_deduct.name
                try:
                    # è¯»å–æ‰£æ¬¾è¡¨ï¼Œä»ç¬¬ä¸‰è¡Œè¯»å–è¡¨å¤´
                    deduction_df = pd.read_excel(tmp_deduct_path, header=2)
                    # è®°å½•è¯»å–åˆ°çš„åˆ—åå’Œå‰å‡ è¡Œæ•°æ®
                    log(f"è¯»å–åˆ°çš„æ‰£æ¬¾è¡¨åˆ—å: {deduction_df.columns.tolist()}", "INFO")
                    log(f"æ‰£æ¬¾è¡¨æ˜ç»† (å‰ 5 è¡Œ): \n{deduction_df.head().to_string()}", "INFO")
                finally:
                    os.unlink(tmp_deduct_path)

                # æ ¡éªŒæ‰£æ¬¾è¡¨å§“ååˆ—
                key_col_found = False
                name_columns = key_identifier_columns
                actual_name_col = None
                for key_col in name_columns:
                    if key_col in deduction_df.columns:
                        key_col_found = True
                        actual_name_col = key_col # è®°å½•æ‰¾åˆ°çš„å§“ååˆ—
                        break
                if not key_col_found:
                    log(f"æ‰£æ¬¾è¡¨å¿…é¡»åŒ…å«ç”¨æˆ·é€‰æ‹©çš„å…³é”®æ ‡è¯†åˆ—ä¸­çš„è‡³å°‘ä¸€ä¸ª ({name_columns})ï¼", "ERROR")
                    st.stop()
                else:
                    log(f"æ‰£æ¬¾æ•°æ®è¯»å–æˆåŠŸï¼Œæ‰¾åˆ°å…³é”®æ ‡è¯†åˆ—: '{actual_name_col}'ã€‚", "INFO")

                # è‡ªåŠ¨ç¡®å®šæ‰£æ¬¾å­—æ®µåˆ—è¡¨
                all_deduction_cols = deduction_df.columns.tolist()
                selected_deduction_fields = [col for col in all_deduction_cols if col != actual_name_col]
                log(f"è‡ªåŠ¨è¯†åˆ«ç”¨äºåˆå¹¶çš„æ‰£æ¬¾å­—æ®µ (å…± {len(selected_deduction_fields)} ä¸ª): {selected_deduction_fields}", "INFO")
                if not selected_deduction_fields:
                     log("è­¦å‘Šï¼šæ‰£æ¬¾è¡¨ä¸­é™¤äº†å§“ååˆ—å¤–æœªæ‰¾åˆ°å…¶ä»–å­—æ®µã€‚", "WARNING")

                # åœ¨è°ƒç”¨process_sheetä¹‹å‰æ·»åŠ é¢„å¤„ç†æ­¥éª¤
                print("\n=== é¢„å¤„ç†æ‰£æ¬¾æ•°æ® ===")
                print(f"æ‰£æ¬¾æ•°æ®å½¢çŠ¶: {deduction_df.shape}")
                print(f"æ‰£æ¬¾æ•°æ®åˆ—: {deduction_df.columns.tolist()}")
                print(f"æ‰£æ¬¾æ•°æ®å‰5è¡Œ:\n{deduction_df.head().to_string()}")

                # ç¡®ä¿æ‰€æœ‰é€‰ä¸­çš„æ‰£æ¬¾å­—æ®µéƒ½æ˜¯æ•°å€¼ç±»å‹
                for field in selected_deduction_fields:
                    if field in deduction_df.columns:
                        deduction_df[field] = pd.to_numeric(deduction_df[field], errors='coerce').fillna(0)
                        print(f"\nå¤„ç†å­—æ®µ {field}:")
                        print(f"æ•°æ®ç±»å‹: {deduction_df[field].dtype}")
                        print(f"éé›¶å€¼æ•°é‡: {(deduction_df[field] != 0).sum()}")
                        print(f"å‰5ä¸ªå€¼: {deduction_df[field].head().to_list()}")

                # --- æ–°å¢ï¼šé¢„è¿‡æ»¤æ˜ å°„è§„åˆ™ --- #
                log("å¼€å§‹é¢„è¿‡æ»¤æ˜ å°„è§„åˆ™...", "INFO")
                filtered_mappings_for_processing = []
                actual_deduction_fields = set(deduction_df.columns)
                # ä½¿ç”¨ä¹‹å‰è·å–çš„ sample_source_fields
                # æ³¨æ„ï¼šsample_source_fields å¯èƒ½æœªåœ¨æ‰€æœ‰åˆ†æ”¯åˆå§‹åŒ–ï¼Œéœ€è¦ç¡®ä¿å®ƒå­˜åœ¨
                if 'sample_source_fields' not in locals():
                     # å¦‚æœ sample_source_fields å› æŸç§åŸå› æœªå®šä¹‰ (ä¾‹å¦‚æ²¡æœ‰ä¸Šä¼ æºæ–‡ä»¶ï¼Œè™½ç„¶å‰é¢æœ‰æ ¡éªŒ)
                     # è¿™é‡Œå¯ä»¥è®¾ç½®ä¸ºç©ºé›†åˆï¼Œæˆ–è€…è®°å½•ä¸€ä¸ªé”™è¯¯ç„¶ååœæ­¢ï¼Ÿè®¾ç½®ä¸ºé›†åˆå¯èƒ½æ›´å®‰å…¨
                     log("è­¦å‘Šï¼šæœªèƒ½è·å–æºæ–‡ä»¶æ ·æœ¬å­—æ®µç”¨äºè§„åˆ™è¿‡æ»¤ï¼Œå°†ä¸æ‰§è¡Œè¿‡æ»¤ã€‚", "WARNING")
                     filtered_mappings_for_processing = current_field_mappings # ä¸è¿‡æ»¤
                else:
                    filtered_rule_count = 0
                    original_mapping_count = 0
                    for rule in current_field_mappings:
                        original_mapping_count += len(rule.get("mappings", []))
                        filtered_rule = rule.copy()
                        filtered_rule["mappings"] = []
                        # --- ä½¿ç”¨ Session State --- #
                        rule_id_for_log = rule.get(st.session_state.single_selected_identity_column, 'æœªçŸ¥è§„åˆ™') # ç”¨äºæ—¥å¿—
                        # --- ç»“æŸä½¿ç”¨ --- #

                        for mapping in rule.get("mappings", []):
                            if "source_field" in mapping:
                                src = mapping["source_field"]
                                # æ¡ä»¶ï¼šæºå­—æ®µåœ¨æ‰£æ¬¾è¡¨å­˜åœ¨ ä¸” åœ¨æºæ–‡ä»¶æ ·æœ¬ä¸­ä¸å­˜åœ¨
                                if src in actual_deduction_fields and src not in sample_source_fields:
                                    log(f"  - è¿‡æ»¤æ‰è§„åˆ™ '{rule_id_for_log}' ä¸­çš„æ— æ•ˆæ˜ å°„: æº '{src}' ä»…å­˜åœ¨äºæ‰£æ¬¾è¡¨ã€‚", "DEBUG")
                                    filtered_rule_count += 1
                                    continue # è·³è¿‡è¿™ä¸ªæ˜ å°„
                                else:
                                    filtered_rule["mappings"].append(mapping) # ä¿ç•™æœ‰æ•ˆæ˜ å°„
                            elif "source_fields" in mapping:
                                filtered_rule["mappings"].append(mapping) # ä¿ç•™å¤æ‚æ˜ å°„
                            else:
                                # å¦‚æœæ˜ å°„æ ¼å¼æœªçŸ¥ï¼Œä¹Ÿä¿ç•™ï¼Ÿæˆ–è€…è­¦å‘Šï¼Ÿæš‚æ—¶ä¿ç•™
                                filtered_rule["mappings"].append(mapping)

                        filtered_mappings_for_processing.append(filtered_rule)
                    log(f"æ˜ å°„è§„åˆ™é¢„è¿‡æ»¤å®Œæˆã€‚å…±è¿‡æ»¤æ‰ {filtered_rule_count} ä¸ªæ— æ•ˆçš„ç®€å•æ˜ å°„ã€‚", "INFO")
                # --- ç»“æŸé¢„è¿‡æ»¤ --- #

                # 3. å¤„ç†æ¯ä¸ªæºæ–‡ä»¶
                all_results = []
                has_error = False
                # --- ä½¿ç”¨ Session State --- #
                identity_column_to_use = st.session_state.single_selected_identity_column
                log(f"å¼€å§‹é€ä¸ªå¤„ç† {len(source_files)} ä¸ªæºæ–‡ä»¶... (ä½¿ç”¨ '{identity_column_to_use}' å­—æ®µåŒ¹é…)", "INFO")
                # --- ç»“æŸä½¿ç”¨ --- #
                with st.spinner(f"æ­£åœ¨å¤„ç† {len(source_files)} ä¸ªæºæ–‡ä»¶..."):
                    for i, uploaded_file in enumerate(source_files):
                        log(f"[{i+1}/{len(source_files)}] å¤„ç†æ–‡ä»¶: {uploaded_file.name}", "INFO")
                        tmp_source_path = None
                        try:
                            with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp_source:
                                tmp_source.write(uploaded_file.getvalue())
                                tmp_source_path = tmp_source.name

                            # --- BEGIN: Add logging for source data before processing ---
                            try:
                                # Attempt to read source file to log info (need to find header)
                                preview_df_for_header = pd.read_excel(tmp_source_path, header=None, nrows=20) # Read first 20 rows to find header
                                # --- ä¿®æ”¹ï¼šä½¿ç”¨ key_identifier_columns --- #
                                header_row_source = next((idx for idx, row in preview_df_for_header.iterrows() if any((key_col in str(cell)) for cell in row.astype(str) for key_col in key_identifier_columns)), None)
                                # --- ç»“æŸä¿®æ”¹ --- #

                                if header_row_source is not None:
                                    df_source_preview = pd.read_excel(tmp_source_path, header=header_row_source)
                                    log(f"  -> æºæ–‡ä»¶ [{uploaded_file.name}] è¯»å–æˆåŠŸ (ä½¿ç”¨ {key_identifier_columns} æ£€æµ‹åˆ°è¡¨å¤´è¡Œ: {header_row_source + 1})ï¼Œå‡†å¤‡é€å…¥ process_sheet...", "INFO") # ä¿®æ”¹æ—¥å¿—
                                    log(f"     æºæ–‡ä»¶åˆ—å: {df_source_preview.columns.tolist()}", "INFO")
                                    log(f"     æºæ–‡ä»¶æ•°æ® (å‰ 5 è¡Œ):\\n{df_source_preview.head().to_string()}", "INFO")
                                else:
                                    log(f"  -> è­¦å‘Š: æœªèƒ½åœ¨æºæ–‡ä»¶ [{uploaded_file.name}] å‰ 20 è¡Œæ‰¾åˆ° {key_identifier_columns} ä¸­çš„ä»»ä½•ä¸€ä¸ªä½œä¸ºè¡¨å¤´ï¼Œæ— æ³•è®°å½•æºæ•°æ®è¯¦æƒ…ã€‚", "WARNING") # ä¿®æ”¹æ—¥å¿—
                                    # Optionally, proceed without preview logging or stop? For now, just warn.
                            except Exception as read_err:
                                 log(f"  -> è­¦å‘Š: å°è¯•è¯»å–æºæ–‡ä»¶ [{uploaded_file.name}] è¿›è¡Œæ—¥å¿—è®°å½•æ—¶å‡ºé”™: {read_err}", "WARNING")
                            # --- END: Add logging for source data before processing ---

                            # --- BEGIN: Add simulated merge for diagnostics ---
                            if 'df_source_preview' in locals() and header_row_source is not None: # Ensure preview was read
                                try:
                                    # --- ä¿®æ”¹ï¼šä½¿ç”¨ key_identifier_columns å’Œ actual_name_col --- #
                                    source_key_to_use = None
                                    if actual_name_col in df_source_preview.columns: # ä¼˜å…ˆä½¿ç”¨æ‰£æ¬¾è¡¨æ‰¾åˆ°çš„é‚£ä¸ª
                                        source_key_to_use = actual_name_col
                                    else: # å¦åˆ™æŸ¥æ‰¾ç¬¬ä¸€ä¸ªåœ¨æºè¡¨ä¸­çš„ç”¨æˆ·é€‰æ‹©çš„å…³é”®åˆ—
                                         source_key_to_use = next((col for col in key_identifier_columns if col in df_source_preview.columns), None)

                                    if source_key_to_use and actual_name_col: # ç¡®ä¿ä¸¤è¾¹éƒ½æœ‰å¯ç”¨çš„é”®
                                        log(f"  -> æ‰§è¡Œæ¨¡æ‹Ÿåˆå¹¶ (æº: {uploaded_file.name}, æ‰£æ¬¾è¡¨) on: æº='{source_key_to_use}', æ‰£æ¬¾='{actual_name_col}'...", "INFO") # ä¿®æ”¹æ—¥å¿—
                                        simulated_merge = pd.merge(df_source_preview, deduction_df, left_on=source_key_to_use, right_on=actual_name_col, how='left', suffixes=('', '_æ‰£æ¬¾')) # ä½¿ç”¨ left_on/right_on
                                        log(f"     æ¨¡æ‹Ÿåˆå¹¶ç»“æœåˆ—å: {simulated_merge.columns.tolist()}", "INFO")
                                        log(f"     æ¨¡æ‹Ÿåˆå¹¶ç»“æœæ•°æ® (å‰ 5 è¡Œ):\\n{simulated_merge.head().to_string()}", "INFO")
                                    else:
                                        log(f"  -> è­¦å‘Š: æ— æ³•æ‰§è¡Œæ¨¡æ‹Ÿåˆå¹¶ï¼Œæºæ–‡ä»¶({key_identifier_columns})æˆ–æ‰£æ¬¾è¡¨({actual_name_col})ç¼ºå°‘æœ‰æ•ˆçš„å…¬å…±æˆ–æŒ‡å®šå…³é”®åˆ—ã€‚", "WARNING") # ä¿®æ”¹æ—¥å¿—
                                    # --- ç»“æŸä¿®æ”¹ --- #
                                except Exception as merge_err:
                                    log(f"  -> é”™è¯¯: æ‰§è¡Œæ¨¡æ‹Ÿåˆå¹¶æ—¶å‡ºé”™: {merge_err}", "ERROR")
                            else:
                                 log(f"  -> è·³è¿‡æ¨¡æ‹Ÿåˆå¹¶ï¼Œå› ä¸ºæœªèƒ½æˆåŠŸè¯»å–æºæ–‡ä»¶é¢„è§ˆã€‚", "INFO")
                            # --- END: Add simulated merge for diagnostics ---

                            # æ·»åŠ è°ƒç”¨ process_sheet çš„æ—¥å¿—
                            log(f"  -> è°ƒç”¨æ ¸å¿ƒå¤„ç†å‡½æ•° process_sheet...", "INFO")
                            result_df = process_sheet(
                                tmp_source_path,
                                deduction_df,
                                filtered_mappings_for_processing,
                                selected_deduction_fields,
                                # --- ä½¿ç”¨ Session State --- #
                                identity_column_to_use,
                                identity_column_to_use # NOTE: Passing identity key twice? Check process_sheet definition if intended.
                                # --- ç»“æŸä½¿ç”¨ --- #
                             )
                            log(f"  <- process_sheet è¿”å›ï¼Œç»“æœè¡Œæ•°: {len(result_df) if result_df is not None else 'None'}", "INFO")

                            # --- BEGIN: Add logging for result data after processing ---
                            if result_df is not None and not result_df.empty:
                                 log(f"     process_sheet è¿”å› [{uploaded_file.name}] åˆ—å: {result_df.columns.tolist()}", "INFO")
                                 log(f"     process_sheet è¿”å› [{uploaded_file.name}] æ•°æ® (å‰ 5 è¡Œ):\\n{result_df.head().to_string()}", "INFO")
                                 all_results.append(result_df)
                                 log(f"[{i+1}/{len(source_files)}] æ–‡ä»¶ {uploaded_file.name} å¤„ç†æˆåŠŸã€‚", "SUCCESS")
                            # --- END: Add logging for result data after processing ---
                            else:
                                 # Keep original warning log if result is None or empty
                                 log(f"[{i+1}/{len(source_files)}] æ–‡ä»¶ {uploaded_file.name} æœªè¿”å›æœ‰æ•ˆæ•°æ® (å¯èƒ½æ— åŒ¹é…è¡Œæˆ–å¤„ç†é”™è¯¯)ã€‚", "WARNING")

                        except ValueError as ve: # æ•è· process_sheet è¿”å›çš„ç‰¹å®šé”™è¯¯ï¼Ÿè¿˜æ˜¯å†…éƒ¨å¤„ç†ï¼Ÿ
                            # å‡è®¾ process_sheet å†…éƒ¨å·²æ‰“å°é”™è¯¯ï¼Œè¿™é‡Œåªè®°å½•å¤±è´¥
                            log(f"[{i+1}/{len(source_files)}] æ–‡ä»¶ {uploaded_file.name} å¤„ç†å¤±è´¥ã€‚", "ERROR")
                            has_error = True
                        except Exception as e:
                            log(f"[{i+1}/{len(source_files)}] å¤„ç†æ–‡ä»¶ {uploaded_file.name} æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", "ERROR")
                            has_error = True
                        finally:
                            if tmp_source_path and os.path.exists(tmp_source_path):
                               os.unlink(tmp_source_path)
                        if has_error:
                             log(f"å› å¤„ç†æ–‡ä»¶ {uploaded_file.name} æ—¶å‘ç”Ÿé”™è¯¯ï¼Œå¤„ç†ä¸­æ­¢ã€‚", "ERROR")
                             break # ä¿æŒä¸­æ­¢é€»è¾‘

                # 4. åˆå¹¶ä¸æ ¼å¼åŒ–
                if not has_error and all_results:
                    log("æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼Œå¼€å§‹åˆå¹¶ {len(all_results)} ä¸ªç»“æœ...", "INFO")
                    with st.spinner("åˆå¹¶ç»“æœå¹¶æ ¼å¼åŒ–è¾“å‡º..."):
                        tmp_processed_path = None
                        output_path = None
                        try:
                            combined_df = pd.concat(all_results, ignore_index=True)
                            log("ç»“æœåˆå¹¶å®Œæˆï¼Œæ€»è¡Œæ•°: {len(combined_df)}", "INFO")

                            if file_template and template_fields:
                                # æ¢å¤ä¸ºä¸¥æ ¼æŒ‰æ¨¡æ¿ reindexï¼Œä¸¢å¼ƒä¸åœ¨æ¨¡æ¿ä¸­çš„åˆ—
                                log(f"æ ¹æ®æ¨¡æ¿æ–‡ä»¶çš„ {len(template_fields)} ä¸ªå­—æ®µä¸¥æ ¼ç­›é€‰å’Œæ’åºè¾“å‡ºåˆ—...", "INFO")
                                combined_df = combined_df.reindex(columns=template_fields)
                                # æ³¨æ„ï¼šå¦‚æœ template_fields åŒ…å« combined_df ä¸­æ²¡æœ‰çš„åˆ—ï¼Œreindex ä¼šæ·»åŠ å®ƒä»¬å¹¶å¡«å…… NaN
                            else:
                                 log("æœªæä¾›æ¨¡æ¿æ–‡ä»¶æˆ–è¯»å–å¤±è´¥ï¼ŒæŒ‰åŸå§‹å¤„ç†é¡ºåºè¾“å‡ºæ‰€æœ‰åˆ—ã€‚", "INFO")

                            log("ä¿å­˜å¤„ç†ç»“æœåˆ°ä¸´æ—¶æ–‡ä»¶...", "INFO")
                            with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp_processed:
                                combined_df.to_excel(tmp_processed.name, index=False)
                                tmp_processed_path = tmp_processed.name

                            output_filename = f"{unit_name}_{salary_date.strftime('%Y%m')}_å·¥èµ„å‘æ”¾è¡¨_å·²å¤„ç†.xlsx"
                            output_dir = tempfile.mkdtemp()
                            output_path = os.path.join(output_dir, output_filename)

                            log("å¼€å§‹æ ¼å¼åŒ–è¾“å‡ºæ–‡ä»¶...", "INFO")
                            format_excel_with_styles(tmp_processed_path, output_path, salary_date.year, salary_date.month)
                            log("æ–‡ä»¶æ ¼å¼åŒ–å®Œæˆã€‚", "SUCCESS")

                            # åœ¨åˆå¹¶æ“ä½œåæ·»åŠ æ—¥å¿—
                            log(f"åˆå¹¶åçš„DataFrameåˆ—å: {combined_df.columns.tolist()}", "INFO")
                            log(f"åˆå¹¶åçš„DataFrameæ•°æ® (å‰ 5 è¡Œ):\n{combined_df.head().to_string()}", "INFO")

                            # åœ¨æ‰£æ¬¾æ˜ç»†è®¡ç®—å‰æ·»åŠ æ—¥å¿—
                            log(f"ç”¨äºè®¡ç®—çš„æ‰£æ¬¾æ˜ç»†å­—æ®µ: {selected_deduction_fields}", "INFO")
                            log(f"æ‰£æ¬¾æ˜ç»†å­—æ®µçš„å€¼ (å‰ 5 è¡Œ):\n{combined_df[selected_deduction_fields].head().to_string()}", "INFO")

                            # åœ¨æ‰£æ¬¾æ˜ç»†è®¡ç®—åæ·»åŠ æ—¥å¿—
                            # --- ä¿®æ”¹ï¼šç§»é™¤ç¡¬ç¼–ç è®¿é—® --- #
                            total_deduction_col_name = 'æ‰£å‘åˆè®¡'
                            if total_deduction_col_name in combined_df.columns:
                                log(f"è®¡ç®—åçš„'{total_deduction_col_name}'å’Œå…¶ä»–æ‰£æ¬¾æ˜ç»†å­—æ®µçš„å€¼ (å‰ 5 è¡Œ):\n{combined_df[[total_deduction_col_name] + selected_deduction_fields].head().to_string()}", "INFO")
                            else:
                                log(f"è®¡ç®—åçš„æ‰£æ¬¾æ˜ç»†å­—æ®µçš„å€¼ (æœªæ‰¾åˆ°'{total_deduction_col_name}'åˆ—) (å‰ 5 è¡Œ):\n{combined_df[selected_deduction_fields].head().to_string()}", "INFO")
                            # --- ç»“æŸä¿®æ”¹ --- #

                            # åœ¨æœ€ç»ˆè¾“å‡ºå‰æ·»åŠ æ—¥å¿—
                            log(f"æœ€ç»ˆè¾“å‡ºçš„DataFrameåˆ—å: {combined_df.columns.tolist()}", "INFO")
                            log(f"æœ€ç»ˆè¾“å‡ºçš„DataFrameæ•°æ® (å‰ 5 è¡Œ):\n{combined_df.head().to_string()}", "INFO")

                            # éªŒè¯ç»“æœ
                            print("\n=== éªŒè¯å¤„ç†ç»“æœ ===")
                            print(f"ç»“æœæ•°æ®å½¢çŠ¶: {result_df.shape}")
                            print(f"ç»“æœæ•°æ®åˆ—: {result_df.columns.tolist()}")

                            # æ£€æŸ¥æ‰£æ¬¾å­—æ®µ
                            for field in selected_deduction_fields:
                                if field in result_df.columns:
                                    print(f"\næ£€æŸ¥å­—æ®µ {field}:")
                                    print(f"æ•°æ®ç±»å‹: {result_df[field].dtype}")
                                    print(f"éé›¶å€¼æ•°é‡: {(result_df[field] != 0).sum()}")
                                    print(f"å‰5ä¸ªå€¼: {result_df[field].head().to_list()}")
                                else:
                                    print(f"\nè­¦å‘Š: å­—æ®µ {field} ä¸åœ¨ç»“æœæ•°æ®ä¸­")

                            # æ£€æŸ¥å§“ååˆ—çš„åŒ¹é…æƒ…å†µ
                            # --- ä¿®æ”¹ï¼šä½¿ç”¨ key_identifier_columns --- #
                            common_key_for_validation = None
                            for key_col in key_identifier_columns:
                                if key_col in result_df.columns and key_col in deduction_df.columns:
                                    common_key_for_validation = key_col
                                    break

                            if common_key_for_validation:
                                print(f"\n=== ä½¿ç”¨å…³é”®åˆ— '{common_key_for_validation}' æ£€æŸ¥å§“ååŒ¹é…æƒ…å†µ ===")
                                source_names = set(result_df[common_key_for_validation].dropna().unique())
                                deduction_names = set(deduction_df[common_key_for_validation].dropna().unique())
                                matched_names = source_names.intersection(deduction_names)
                                print(f"æºæ–‡ä»¶ä¸­çš„ '{common_key_for_validation}' æ•°é‡: {len(source_names)}")
                                print(f"æ‰£æ¬¾è¡¨ä¸­çš„ '{common_key_for_validation}' æ•°é‡: {len(deduction_names)}")
                                print(f"æˆåŠŸåŒ¹é…çš„ '{common_key_for_validation}' æ•°é‡: {len(matched_names)}")
                                if len(matched_names) < len(source_names):
                                    print(f"è­¦å‘Š: éƒ¨åˆ†æºæ–‡ä»¶ä¸­çš„ '{common_key_for_validation}' æœªèƒ½åŒ¹é…åˆ°æ‰£æ¬¾æ•°æ®")
                                    print("æœªåŒ¹é…ç¤ºä¾‹:")
                                    unmatched = source_names - matched_names
                                    print(list(unmatched)[:5])
                            else:
                                print(f"\nè­¦å‘Š: æœªèƒ½åœ¨ç»“æœè¡¨å’Œæ‰£æ¬¾è¡¨ä¸­æ‰¾åˆ°å…±åŒçš„å…³é”®æ ‡è¯†åˆ— ({key_identifier_columns}) ç”¨äºåŒ¹é…éªŒè¯ã€‚")
                            # --- ç»“æŸä¿®æ”¹ --- #

                            # 5. æä¾›ä¸‹è½½
                            # st.success(f"ğŸ‰ å¤„ç†å®Œæˆï¼...") # ç”± log æ›¿ä»£
                            log(f"å¤„ç†æˆåŠŸå®Œæˆï¼æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆï¼š{output_filename}", "SUCCESS")
                            with open(output_path, "rb") as fp:
                                st.download_button(
                                    label="ğŸ“¥ ä¸‹è½½æœ€ç»ˆæŠ¥å‘Š",
                                    data=fp,
                                    file_name=output_filename,
                                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                    key="download_report"
                                )

                        except Exception as e:
                            log(f"åˆå¹¶æˆ–æ ¼å¼åŒ– Excel æ–‡ä»¶æ—¶å‡ºé”™: {e}", "ERROR")
                            has_error = True # ç¡®ä¿æ ‡è®°é”™è¯¯
                        finally:
                            if tmp_processed_path and os.path.exists(tmp_processed_path):
                                os.unlink(tmp_processed_path)

                elif not all_results and not has_error:
                     log("æœªç”Ÿæˆä»»ä½•æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥æºæ–‡ä»¶å†…å®¹å’Œæ˜ å°„è§„åˆ™ã€‚", "WARNING")
                elif has_error:
                    log("å¤„ç†å› å‘ç”Ÿé”™è¯¯è€Œä¸­æ­¢ã€‚", "ERROR")

            except Exception as e:
                log(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿæ— æ³•æ¢å¤çš„ä¸¥é‡é”™è¯¯: {e}", "ERROR")
                # ç¡®ä¿æ¸…ç†å¯èƒ½é—ç•™çš„ä¸´æ—¶æ–‡ä»¶
                if 'tmp_deduct_path' in locals() and os.path.exists(tmp_deduct_path):
                    os.unlink(tmp_deduct_path)
                if 'tmp_source_path' in locals() and os.path.exists(tmp_source_path):
                    os.unlink(tmp_source_path)
                if 'tmp_processed_path' in locals() and os.path.exists(tmp_processed_path):
                    os.unlink(tmp_processed_path)

        else:
            log("è¾“å…¥æ ¡éªŒå¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šä¼ çš„æ–‡ä»¶å’Œé…ç½®ã€‚", "ERROR")

# å¯ä»¥æ·»åŠ é¡µè„šç­‰ä¿¡æ¯
st.markdown("---")
st.caption("Â© æˆéƒ½é«˜æ–°åŒºè´¢æ”¿é‡‘èå±€")
