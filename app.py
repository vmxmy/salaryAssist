import streamlit as st
import pandas as pd
import tempfile
import os
from datetime import datetime
from fiscal_report_full_script import process_sheet, format_excel_with_styles
import json
import matplotlib.pyplot as plt
import numpy as np # ç¡®ä¿å¯¼å…¥ numpy

# --- Matplotlib ä¸­æ–‡æ˜¾ç¤ºè®¾ç½® ---
# ä½¿ç”¨ä»ç³»ç»Ÿä¸­æ‰¾åˆ°çš„å­—ä½“ï¼Œä¼˜å…ˆ Lantinghei SC
plt.rcParams['font.sans-serif'] = ['Lantinghei SC', 'SimSong', 'Kaiti SC', 'Songti SC', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False
# --- ç»“æŸè®¾ç½® ---

# --- åˆå§‹åŒ– Session State ---
if 'log_messages' not in st.session_state:
    st.session_state.log_messages = []
if 'mapping_data' not in st.session_state:
    st.session_state.mapping_data = None
if 'mapping_valid' not in st.session_state:
    st.session_state.mapping_valid = None # None: æœªä¸Šä¼ , True: æœ‰æ•ˆ, False: æ— æ•ˆ
# --- æ–°å¢ Session State for Single Select --- #
if 'single_selected_identity_column' not in st.session_state:
    st.session_state.single_selected_identity_column = None
# --- ç»“æŸåˆå§‹åŒ– ---

# --- æ—¥å¿—è®°å½•å‡½æ•° ---
def log(message, level="INFO"):
    now = datetime.now().strftime("%H:%M:%S")
    level_icon_map = {"INFO": "â„¹ï¸", "WARNING": "âš ï¸", "ERROR": "âŒ", "SUCCESS": "âœ…"}
    icon = level_icon_map.get(level, "â–ªï¸")
    log_level_class = f"log-{level.lower()}"

    # Wrap prefix and message in spans for styling
    formatted_message = f"<span class='log-prefix'>{now} {icon}</span> <span class='{log_level_class}'>{message}</span>"

    st.session_state.log_messages.append(formatted_message)
    # æ³¨æ„ï¼šæ—¥å¿—åœ¨ä¾§è¾¹æ çš„æ›´æ–°é€šå¸¸åœ¨æ•´ä¸ªæŒ‰é’®è„šæœ¬æ‰§è¡Œå®Œåå‘ç”Ÿ
# --- ç»“æŸæ—¥å¿—å‡½æ•° ---

st.set_page_config(layout="wide", page_title="è´¢æ”¿å·¥èµ„å¤„ç†ç³»ç»Ÿ")

# --- NEW CSS for Modern Minimalist Style ---
modern_minimalist_css = """
<style>
/* Import Google Font */
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');

/* Global Styles */
html, body, [class*="st-"] {{
    font-family: 'Inter', sans-serif;
    background-color: #F8F9FA; /* Light grey background */
    color: #212529; /* Dark text */
}}

/* Sidebar Styles */
[data-testid="stSidebar"] {{
    background-color: #FFFFFF; /* White sidebar */
    border-right: 1px solid #E0E0E0; /* Subtle border */
    box-shadow: 2px 0px 5px rgba(0, 0, 0, 0.05); /* Slight shadow */
}}

/* Titles and Headers */
h1, h2, h3, h4, h5, h6 {{
    font-family: 'Inter', sans-serif;
    font-weight: 600; /* Bolder headers */
}}

/* Custom simple subheader style (replaces old .custom-subheader) */
.simple-subheader {{
    font-size: 1.5em; /* H3 size equivalent */
    font-weight: 600;
    margin-top: 25px;
    margin-bottom: 15px;
    border-bottom: 1px solid #E0E0E0; /* Subtle underline */
    padding-bottom: 8px;
}}

/* Input Controls */
[data-testid="stTextInput"] input,
[data-testid="stDateInput"] input,
[data-testid="stSelectbox"] div[role="combobox"] {{
    border: 1px solid #ced4da;
    border-radius: 0.3rem;
    background-color: #FFFFFF;
}}

/* File Uploader Adjustments */
[data-testid="stFileUploader"] {{
    /* Add some space */
    margin-bottom: 10px;
}}

/* Primary Button Styling */
button[kind="primary"] {{
    background-color: #0d6efd; /* Primary blue */
    color: white;
    padding: 0.75rem 1.25rem; /* Moderate padding */
    font-size: 1rem; /* Standard font size */
    font-weight: 600;
    border-radius: 0.3rem; /* Consistent rounding */
    border: none; /* Remove default border */
    transition: background-color 0.2s ease-in-out; /* Hover effect */
}}

button[kind="primary"]:hover {{
    background-color: #0b5ed7; /* Darker blue on hover */
}}

/* --- æ›´æ–°ï¼šä¸ºé»˜è®¤æŒ‰é’®æ·»åŠ ç°ä»£è½®å»“æ ·å¼ --- */
button:not([kind="primary"]) {{
    background-color: transparent;
    color: #198754; /* Bootstrap success green */
    border: 1px solid #198754;
    padding: 0.75rem 1.25rem; /* Match primary button padding */
    font-size: 1rem; /* Match primary font size */
    font-weight: 600; /* Match primary font weight */
    border-radius: 0.5rem; /* Softer corners */
    transition: background-color 0.2s ease-in-out, color 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
}}

button:not([kind="primary"]):hover {{
    background-color: #198754; /* Fill with green on hover */
    color: white;
    border-color: #198754;
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.08);
}}
/* --- ç»“æŸæ›´æ–° --- */

/* Log Message Styling */
.log-info {{ color: #0dcaf0; }} /* Cyan info */
.log-warning {{ color: #ffc107; }} /* Yellow warning */
.log-error {{ color: #dc3545; font-weight: bold; }} /* Red, bold error */
.log-success {{ color: #198754; }} /* Green success */

/* Styling for the log timestamp and icon */
.log-prefix {{
    color: #6c757d; /* Grey for timestamp/icon */
    margin-right: 5px;
}}

/* Adjust log container in sidebar */
[data-testid="stSidebar"] [data-testid="stExpander"] [data-testid="stVerticalBlock"] {{
     /* Potentially adjust padding or spacing if needed */
}}

</style>
"""
st.markdown(modern_minimalist_css, unsafe_allow_html=True)

# --- æ—§ CSS (æ³¨é‡Šæ‰æˆ–åˆ é™¤) ---
# styled_header_css = """
# <style>
# .custom-subheader {
#     border-left: 5px solid #1E90FF; /* Dodger blue left border */
#     background-color: #F0F8FF; /* Alice blue background */
#     padding: 10px 15px;      /* Padding around text */
#     margin-top: 20px;         /* Space above */
#     margin-bottom: 15px;      /* Space below */
#     border-radius: 5px;       /* Rounded corners */
#     font-size: 1.75em;        /* H3 font size */
#     line-height: 1.4;         /* Adjust line height */
#     font-weight: 600;         /* Make it bolder like headers */
# }
# /* Ensure the text inside aligns well, prevent potential default margins */
# .custom-subheader h3 {
#     margin: 0;
#     padding: 0;
#     line-height: inherit; /* Inherit line height */
#     font-size: inherit; /* Inherit font size */
#     font-weight: inherit; /* Inherit font weight */
# }
#
# /* Target Streamlit's primary button to make it larger */
# button[kind="primary"] {
#     padding: 15px 30px; /* Further increase padding */
#     font-size: 2.4em;  /* Further increase font size */
#     font-weight: bold; /* Ensure font is bold */
# }
# </style>
# """
# st.markdown(styled_header_css, unsafe_allow_html=True)

# --- ä¸»ç•Œé¢ ---
st.title("å·¥èµ„è¡¨åˆå¹¶AIåŠ©æ‰‹")

# --- è¡¨å•è¾“å…¥åŒºåŸŸ ---
st.sidebar.title("é«˜æ–°åŒºè´¢é‡‘å±€ç»¼åˆå¤„")
st.sidebar.header("ğŸ”§ å‚æ•°è®¾ç½®")

# è‡ªå®šä¹‰å•ä½åç§°
unit_name = st.sidebar.text_input("å•ä½åç§°", value="é«˜æ–°åŒºè´¢æ”¿å±€")

# æ—¥æœŸæ§ä»¶ï¼ˆé»˜è®¤å½“å‰æœˆä»½ï¼‰
def_year = datetime.today().year
def_month = datetime.today().month

salary_date = st.sidebar.date_input("å·¥èµ„è¡¨æ—¥æœŸï¼ˆç”¨äºæ ‡é¢˜æ ï¼‰", value=datetime(def_year, def_month, 1), format="YYYY-MM-DD")


# æ–‡ä»¶ä¸Šä¼ 
st.markdown("<p class='simple-subheader'>ğŸ“ ä¸Šä¼ æ‰€éœ€æ–‡ä»¶</p>", unsafe_allow_html=True)

col1, col2 = st.columns(2)
with col1:
    # ä½¿ç”¨ markdown ä½œä¸ºæ ‡ç­¾ï¼Œå¹¶éšè— file_uploader è‡ªå¸¦çš„æ ‡ç­¾
    st.caption("ğŸ“ æºæ•°æ®å·¥èµ„è¡¨ï¼ˆæ”¯æŒå¤šæ–‡ä»¶ï¼‰")
    source_files = st.file_uploader("source_uploader", type=["xlsx"], accept_multiple_files=True, label_visibility="collapsed", key="source_uploader")

    st.caption("ğŸ“ å¯¼å‡ºè¡¨å­—æ®µæ¨¡æ¿")
    file_template = st.file_uploader("template_uploader", type=["xlsx"], label_visibility="collapsed", key="template_uploader")

with col2:
    st.caption("ğŸ“ æ‰£æ¬¾é¡¹è¡¨ï¼ˆå«å§“å+å„é¡¹æ‰£æ¬¾ï¼‰")
    file_deductions = st.file_uploader("deductions_uploader", type=["xlsx"], label_visibility="collapsed", key="deductions_uploader")

    st.caption("ğŸ“ å­—æ®µæ˜ å°„è§„åˆ™ ï¼ˆJSONæ ¼å¼ï¼‰")
    file_mapping = st.file_uploader("mapping_uploader", type=["json"], label_visibility="collapsed", key="mapping_uploader")

# --- JSON æ ¡éªŒé€»è¾‘ ---
# æ¯æ¬¡è„šæœ¬è¿è¡Œæ—¶éƒ½é‡æ–°æ ¡éªŒä¸Šä¼ çš„æ–‡ä»¶çŠ¶æ€
mapping_validation_placeholder = st.empty() # ç”¨äºæ˜¾ç¤ºæ ¡éªŒç»“æœæ¶ˆæ¯
temp_mapping_data = None
if file_mapping is not None:
    try:
        # é‡ç½®æ–‡ä»¶è¯»å–æŒ‡é’ˆ
        file_mapping.seek(0)
        # å°è¯•è§£æ JSON
        temp_mapping_data = json.load(file_mapping)
        # åŸºæœ¬ç»“æ„æ£€æŸ¥ (ç¡®ä¿é¡¶å±‚æ˜¯å¯¹è±¡ï¼Œä¸”åŒ…å« 'field_mappings' åˆ—è¡¨)
        if isinstance(temp_mapping_data, dict) and isinstance(temp_mapping_data.get('field_mappings'), list):
            # åªæœ‰å½“æ–°ä¸Šä¼ çš„æ–‡ä»¶æœ‰æ•ˆæ—¶æ‰æ›´æ–° session_state
            st.session_state.mapping_data = temp_mapping_data
            st.session_state.mapping_valid = True
            mapping_validation_placeholder.success("âœ… æ˜ å°„æ–‡ä»¶ JSON æœ‰æ•ˆã€‚")
        else:
            st.session_state.mapping_valid = False # æ ‡è®°ä¸ºæ— æ•ˆï¼Œä½†ä¸æ¸…é™¤å¯èƒ½å­˜åœ¨çš„æ—§æœ‰æ•ˆæ•°æ®
            mapping_validation_placeholder.error("âŒ JSON æ–‡ä»¶é¡¶å±‚ç»“æ„é”™è¯¯ï¼šéœ€è¦åŒ…å« 'field_mappings' åˆ—è¡¨ã€‚")
            # å¯é€‰ï¼šæ¸…é™¤æ—§æ•°æ® st.session_state.mapping_data = None

    except json.JSONDecodeError as e:
        st.session_state.mapping_valid = False
        mapping_validation_placeholder.error(f"âŒ æ˜ å°„æ–‡ä»¶ JSON è¯­æ³•é”™è¯¯ï¼š\nåœ¨è¡Œ {e.lineno} åˆ— {e.colno} é™„è¿‘: {e.msg}")
        # å¯é€‰ï¼šæ¸…é™¤æ—§æ•°æ® st.session_state.mapping_data = None
    except Exception as e: # æ•è·å…¶ä»–å¯èƒ½çš„è¯»å–é”™è¯¯
        st.session_state.mapping_valid = False
        mapping_validation_placeholder.error(f"âŒ è¯»å–æˆ–è§£ææ˜ å°„æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        # å¯é€‰ï¼šæ¸…é™¤æ—§æ•°æ® st.session_state.mapping_data = None
else:
    # å¦‚æœ file_mapping æ§ä»¶ä¸­æ²¡æœ‰æ–‡ä»¶ï¼ˆä¾‹å¦‚ç”¨æˆ·æ¸…é™¤äº†ä¸Šä¼ ï¼‰ï¼Œä¹Ÿåº”æ›´æ–°çŠ¶æ€
    # å¦‚æœå¸Œæœ›ä¿ç•™ä¸Šæ¬¡æœ‰æ•ˆä¸Šä¼ çš„æ•°æ®ï¼Œå¯ä»¥æ³¨é‡Šæ‰ä¸‹é¢ä¸¤è¡Œ
    # st.session_state.mapping_data = None
    st.session_state.mapping_valid = None # è®¾ä¸º None è¡¨ç¤ºæœªä¸Šä¼ çŠ¶æ€
# --- ç»“æŸ JSON æ ¡éªŒ ---

# --- æ–°å¢ï¼šç‹¬ç«‹çŠ¶æ€æ¡ --- #
st.sidebar.subheader("ğŸ“Š æ–‡ä»¶ä¸Šä¼ çŠ¶æ€")

required_files_status = {
    "æºæ•°æ®å·¥èµ„è¡¨": bool(source_files),
    "æ‰£æ¬¾é¡¹è¡¨": bool(file_deductions),
    "å­—æ®µæ˜ å°„è§„åˆ™": bool(file_mapping and st.session_state.get('mapping_valid') is True)
}
optional_files_status = {
    "å¯¼å‡ºè¡¨å­—æ®µæ¨¡æ¿": bool(file_template)
}

# --- æ–°å¢ï¼šç‹¬ç«‹çŠ¶æ€æ¡ --- #
st.sidebar.markdown("**å¿…éœ€æ–‡ä»¶:**")
for file_name, is_uploaded in required_files_status.items():
    if is_uploaded:
        st.sidebar.success(f"âœ… {file_name}")
    else:
        st.sidebar.warning(f"ğŸŸ¡ {file_name} (æœªä¸Šä¼ )")

st.sidebar.markdown("**å¯é€‰æ–‡ä»¶:**")
for file_name, is_uploaded in optional_files_status.items():
    if is_uploaded:
        st.sidebar.success(f"âœ… {file_name} (å¯é€‰)")
    else:
        st.sidebar.info(f"âšªï¸ {file_name} (å¯é€‰, æœªä¸Šä¼ )")
# --- ç»“æŸæ–°å¢ --- #

st.sidebar.markdown("---")

# --- æ—¥å¿—æ˜¾ç¤ºåŒºåŸŸ ---
with st.sidebar.expander("ğŸ“„ å¤„ç†æ—¥å¿—", expanded=True):
    log_container = st.container(height=300) # å›ºå®šé«˜åº¦å¯æ»šåŠ¨å®¹å™¨
    with log_container:
        for message in st.session_state.log_messages:
            st.markdown(message, unsafe_allow_html=True) # Markdown is now expected to contain spans like <span class='log-info'>...</span>
# --- ç»“æŸæ—¥å¿—æ˜¾ç¤º ---

# ä» session_state è·å–æ•°æ®ï¼Œå¦‚æœæ— æ•ˆæˆ–ä¸º None åˆ™ç”¨ç©ºå­—å…¸
mapping_data_from_state = st.session_state.get('mapping_data') or {}
field_mappings = mapping_data_from_state.get('field_mappings', [])

# --- ç¡®ä¿ template_fields å’Œ sample_source_fields åœ¨æ­¤å¤„è¢«æ— æ¡ä»¶åˆå§‹åŒ– ---
template_fields = []
sample_source_fields = set()
# --- ç»“æŸæ— æ¡ä»¶åˆå§‹åŒ– ---

# æ¨¡æ¿å­—æ®µåé¢„å–
if file_template:
    try:
        file_template.seek(0)
        template_df_header = pd.read_excel(file_template, skiprows=2, nrows=1)
        template_fields = template_df_header.columns.tolist()
    except Exception as e:
        st.warning(f"æ¨¡æ¿å­—æ®µè¯»å–å¤±è´¥ï¼š{e}")

# æºæ•°æ®å­—æ®µç¤ºä¾‹æ”¶é›†
if source_files:
    try:
        uploaded_source_file = source_files[0]
        uploaded_source_file.seek(0)
        source_preview = pd.read_excel(uploaded_source_file, nrows=10, header=None)
        # ä¿®å¤æ‹¬å·å’Œé€»è¾‘ï¼šæŸ¥æ‰¾åŒ…å«'å§“å'æˆ–'äººå‘˜å§“å'çš„è¡Œ
        # --- ä¿®æ”¹ï¼šä½¿ç”¨é»˜è®¤å…³é”®å­—è¿›è¡Œé¦–æ¬¡æ£€æµ‹ä»¥å¡«å……é€‰é¡¹ --- #
        default_keywords_for_options = ["å§“å", "äººå‘˜å§“å"]
        header_row = next((i for i, row in source_preview.iterrows() if any((key_col in str(cell)) for cell in row.astype(str) for key_col in default_keywords_for_options)), None)
        # --- ç»“æŸä¿®æ”¹ --- #
        if header_row is not None:
            # é‡ç½®æŒ‡é’ˆï¼Œè¯»å–æ­£ç¡®çš„è¡¨å¤´è¡Œ
            uploaded_source_file.seek(0)
            df_source_cols = pd.read_excel(uploaded_source_file, skiprows=header_row, nrows=0).columns.tolist()
            sample_source_fields = set(df_source_cols)
        else:
            st.warning("æ— æ³•åœ¨ç¬¬ä¸€ä¸ªæºæ–‡ä»¶ä¸­è‡ªåŠ¨æ£€æµ‹è¡¨å¤´è¡Œä»¥è·å–ç¤ºä¾‹å­—æ®µã€‚")
    except Exception as e:
        st.warning(f"è¯»å–æºæ•°æ®ç¤ºä¾‹å­—æ®µæ—¶å‡ºé”™: {e}")

# --- æ–°å¢ï¼šå…³é”®æ ‡è¯†åˆ—é€‰æ‹© ---
st.markdown("<p class='simple-subheader'>ğŸ”‘ é€‰æ‹©å…³é”®æ ‡è¯†åˆ—</p>", unsafe_allow_html=True)
pre_selected_keys = [col for col in ["å§“å", "äººå‘˜å§“å"] if col in sample_source_fields]
key_identifier_columns = st.multiselect(
    "é€‰æ‹©ç”¨äºåŒ¹é…å’ŒæŸ¥æ‰¾è¡¨å¤´çš„å…³é”®åˆ—åï¼ˆå¯å¤šé€‰ï¼‰",
    options=sample_source_fields,
    default=pre_selected_keys,
    help="è¿™äº›åˆ—å°†ç”¨äºè‡ªåŠ¨æ£€æµ‹æºæ–‡ä»¶è¡¨å¤´ï¼Œå¹¶ä½œä¸ºåˆå¹¶æ‰£æ¬¾è¡¨æ—¶çš„ä¾æ®ã€‚è¯·è‡³å°‘é€‰æ‹©ä¸€é¡¹ã€‚"
)
# --- ç»“æŸæ–°å¢ --- #

# --- è§„åˆ™åŒ¹é…è®¾ç½® --- #
st.markdown("<p class='simple-subheader'>ğŸ”€ å­—æ®µåŒ¹é…å…³ç³»è®¾ç½®</p>", unsafe_allow_html=True)
# åªä¿ç•™ä¸€ä¸ªé€‰æ‹©æ¡†ï¼Œå› ä¸ºæºæ–‡ä»¶å’Œè§„åˆ™ä½¿ç”¨ç›¸åŒå­—æ®µå
# ä½¿ç”¨ sample_source_fields (ç¡®ä¿å®ƒå·²å®šä¹‰å¹¶å¯èƒ½æ˜¯ set)
source_cols_list = sorted(list(sample_source_fields)) if sample_source_fields else []

# --- æ›¿æ¢ä¸º Multiselect å¹¶æ·»åŠ å¼ºåˆ¶å•é€‰é€»è¾‘ --- #
# å°è¯•é¢„è®¾é»˜è®¤å€¼ (å¦‚æœä¹‹å‰æœ‰é€‰è¿‡)
default_identity = None
if st.session_state.single_selected_identity_column and st.session_state.single_selected_identity_column in source_cols_list:
    default_identity = st.session_state.single_selected_identity_column
elif not st.session_state.single_selected_identity_column:
    # --- ä¿®æ”¹ï¼šä»…å½“ source_cols_list éç©ºæ—¶å°è¯•è®¾ç½®é»˜è®¤å€¼ --- #
    if source_cols_list:
        # å°è¯•æ™ºèƒ½é»˜è®¤ï¼šäººå‘˜èº«ä»½ > å²—ä½ç±»åˆ« > ç¬¬ä¸€ä¸ªé€‰é¡¹
        if "äººå‘˜èº«ä»½" in source_cols_list:
            default_identity = "äººå‘˜èº«ä»½"
        elif "å²—ä½ç±»åˆ«" in source_cols_list:
            default_identity = "å²—ä½ç±»åˆ«"
        elif source_cols_list: # è¿™ä¸€å±‚ elif å…¶å®å¯ä»¥çœç•¥ï¼Œå› ä¸ºå¤–å±‚ if å·²ä¿è¯éç©º
            default_identity = source_cols_list[0]
        # å°†åˆå§‹é»˜è®¤å€¼å­˜å…¥ session_state
        st.session_state.single_selected_identity_column = default_identity
    # å¦‚æœ source_cols_list ä¸ºç©ºï¼Œåˆ™ default_identity ä¿æŒ None, session state ä¹Ÿä¸º None
    # --- ç»“æŸä¿®æ”¹ --- #

# --- ä¿®æ”¹ï¼šç¡®ä¿ default å€¼åœ¨ options ä¸­ --- #
current_selection_in_state = []
if st.session_state.single_selected_identity_column and st.session_state.single_selected_identity_column in source_cols_list:
    current_selection_in_state = [st.session_state.single_selected_identity_column]
# --- ç»“æŸä¿®æ”¹ --- #

selected_identity_list = st.multiselect(
    "é€‰æ‹©ç”¨äºåŒ¹é…è§„åˆ™çš„å­—æ®µå (å•é€‰)", # ä¿®æ”¹æ ‡ç­¾ä»¥æç¤ºå•é€‰
    options=source_cols_list,
    default=current_selection_in_state,
    key="identity_multiselect", # æ·»åŠ  key
    help="é€‰æ‹©æºæ–‡ä»¶å’ŒJSONè§„åˆ™ä¸­éƒ½ä½¿ç”¨çš„é‚£ä¸ªå­—æ®µåè¿›è¡ŒåŒ¹é…ï¼ˆå¦‚ äººå‘˜èº«ä»½, å²—ä½ç±»åˆ«ï¼‰ã€‚è™½ä¸ºå¤šé€‰æ¡†ï¼Œä½†ä»…ç¬¬ä¸€ä¸ªé€‰é¡¹ç”Ÿæ•ˆã€‚"
)

# å¼ºåˆ¶å•é€‰é€»è¾‘
if len(selected_identity_list) == 0:
    if st.session_state.single_selected_identity_column is not None:
        st.session_state.single_selected_identity_column = None
        st.rerun()
elif len(selected_identity_list) == 1:
    if st.session_state.single_selected_identity_column != selected_identity_list[0]:
        st.session_state.single_selected_identity_column = selected_identity_list[0]
        st.rerun()
elif len(selected_identity_list) > 1:
    # å¦‚æœç”¨æˆ·é€‰äº†å¤šä¸ªï¼Œåªä¿ç•™æœ€åä¸€ä¸ªï¼ˆæœ€æ–°çš„é€‰æ‹©ï¼‰
    last_selected = selected_identity_list[-1]
    if st.session_state.single_selected_identity_column != last_selected:
         st.session_state.single_selected_identity_column = last_selected
         st.rerun()
    # å¦åˆ™ï¼Œå¦‚æœæœ€åä¸€ä¸ªå’Œ state ç›¸åŒï¼Œä½†ä»æœ‰å¤šé€‰ï¼Œä¹Ÿéœ€è¦å¼ºåˆ¶åˆ·æ–°å›å•é€‰
    elif len(current_selection_in_state) != 1 or current_selection_in_state[0] != last_selected:
         st.rerun()
# --- ç»“æŸæ›¿æ¢å’Œé€»è¾‘ --- #

# --- ç»“æŸè§„åˆ™åŒ¹é…è®¾ç½® --- #

# --- ç§»åŠ¨ï¼šæ•°æ®æœ‰æ•ˆæ€§æ ¡éªŒåŒºåŸŸ --- #
st.markdown("<p class='simple-subheader'>ï¸âœ… æ•°æ®æœ‰æ•ˆæ€§æ ¡éªŒ</p>", unsafe_allow_html=True)
if st.button("ğŸ” æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§", key="validate_data_btn"):
    validation_placeholder = st.container() # åˆ›å»ºå®¹å™¨æ˜¾ç¤ºç»“æœ
    with validation_placeholder:

        # --- å¼€å§‹å®ç°æ··åˆæ ¡éªŒé€»è¾‘ --- #
        validation_errors = []
        validation_warnings = []

        # B. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸Šä¼  (å¤ç”¨ä¾§è¾¹æ é€»è¾‘ç¨å¾®ä¿®æ”¹)
        if not all(required_files_status.values()):
            validation_errors.append(f"å¿…éœ€æ–‡ä»¶ç¼ºå¤±æˆ–æ— æ•ˆ: {', '.join(name for name, uploaded in required_files_status.items() if not uploaded)}")
        # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦é€‰æ‹©äº†å…³é”®åˆ—å’Œèº«ä»½åˆ—
        if not key_identifier_columns:
             validation_errors.append("è¯·åœ¨ä¸Šæ–¹'å…³é”®æ ‡è¯†åˆ—'æ§ä»¶ä¸­é€‰æ‹©è‡³å°‘ä¸€é¡¹ã€‚") # ä¿®æ”¹æç¤º
        if not selected_identity_list:
             validation_errors.append("è¯·åœ¨ä¸Šæ–¹'ç”¨äºåŒ¹é…è§„åˆ™çš„å­—æ®µå'æ§ä»¶ä¸­é€‰æ‹©ä¸€é¡¹ã€‚") # ä¿®æ”¹æç¤º

        # å¦‚æœåŸºç¡€æ–‡ä»¶æˆ–é€‰æ‹©ç¼ºå¤±ï¼Œåˆ™ä¸ç»§ç»­æ£€æŸ¥
        if validation_errors:
             # H. æ˜¾ç¤ºæœ€ç»ˆæ ¡éªŒç»“æœ (ä»…é”™è¯¯éƒ¨åˆ†)
             st.error("**æ ¡éªŒå¤±è´¥:**\n" + "\n".join(validation_errors))
             if 'validation_passed' in st.session_state: del st.session_state.validation_passed # æ¸…é™¤æ—§çŠ¶æ€

        else:
            try:
                # C. è¯»å–æ‰€æœ‰è¡¨å¤´ä¿¡æ¯
                # æ‰£æ¬¾è¡¨
                actual_deduction_fields = set()
                try:
                    file_deductions.seek(0)
                    deduction_df_headers = pd.read_excel(file_deductions, header=2, nrows=0) # åªè¯»è¡¨å¤´
                    actual_deduction_fields = set(deduction_df_headers.columns)
                except Exception as e:
                    validation_errors.append(f"è¯»å–æ‰£æ¬¾è¡¨è¡¨å¤´å¤±è´¥: {e}")

                # æ¨¡æ¿è¡¨ (å¦‚æœä¸Šä¼ )
                actual_template_fields = []
                template_available = False
                if file_template:
                    try:
                        file_template.seek(0)
                        template_df_header = pd.read_excel(file_template, skiprows=2, nrows=1)
                        actual_template_fields = template_df_header.columns.tolist()
                        template_available = True
                    except Exception as e:
                        validation_warnings.append(f"è¯»å–æ¨¡æ¿è¡¨è¡¨å¤´å¤±è´¥: {e} (ç›®æ ‡å­—æ®µæœ‰æ•ˆæ€§å°†æ— æ³•æ£€æŸ¥)")

                # æ‰€æœ‰æºæ–‡ä»¶
                all_actual_source_fields = set()
                source_read_errors = []
                default_keywords_for_header = ["å§“å", "äººå‘˜å§“å"]
                for i, src_file in enumerate(source_files):
                    try:
                        src_file.seek(0)
                        preview_df = pd.read_excel(src_file, header=None, nrows=20)
                        header_row_idx = next((idx for idx, row in preview_df.iterrows() if any((key in str(cell)) for cell in row.astype(str) for key in default_keywords_for_header)), None)
                        if header_row_idx is not None:
                             src_file.seek(0) # éœ€è¦é‡ç½®æŒ‡é’ˆä»¥æ­£ç¡®è¯»å–åˆ—
                             df_cols = pd.read_excel(src_file, header=header_row_idx, nrows=0).columns.tolist()
                             all_actual_source_fields.update(df_cols)
                        else:
                             source_read_errors.append(f"æ–‡ä»¶ '{src_file.name}' æœªèƒ½è‡ªåŠ¨æ£€æµ‹åˆ°è¡¨å¤´è¡Œ (ä½¿ç”¨é»˜è®¤å…³é”®å­—)ã€‚")
                    except Exception as e:
                         source_read_errors.append(f"è¯»å–æºæ–‡ä»¶ '{src_file.name}' çš„åˆ—åå¤±è´¥: {e}")
                if source_read_errors:
                     validation_warnings.extend(source_read_errors)
                if not all_actual_source_fields:
                     validation_errors.append("æœªèƒ½ä»ä»»ä½•æºæ–‡ä»¶ä¸­æˆåŠŸè¯»å–åˆ—åã€‚")

                # D. è¯»å– JSON è§„åˆ™ (å·²åœ¨å‰é¢åŠ è½½åˆ° st.session_state.mapping_data)
                if 'mapping_data' not in st.session_state or not st.session_state.mapping_data:
                     validation_errors.append("æ— æ³•åŠ è½½ JSON æ˜ å°„è§„åˆ™ã€‚")
                else:
                     field_mappings = st.session_state.mapping_data.get('field_mappings', [])

                     # å¦‚æœå‰é¢æ­¥éª¤æœ‰é”™è¯¯ï¼Œåˆ™åœæ­¢è¿›ä¸€æ­¥æ£€æŸ¥
                     if not validation_errors:
                         # F. æ‰§è¡Œå­—æ®µé‡å¤æ£€æŸ¥
                         common_fields = all_actual_source_fields.intersection(actual_deduction_fields)
                         repeated_non_key_fields = common_fields - set(key_identifier_columns)
                         if repeated_non_key_fields:
                             validation_errors.append(f"å­—æ®µå†²çªï¼šä»¥ä¸‹å­—æ®µåŒæ—¶å­˜åœ¨äºæºæ–‡ä»¶å’Œæ‰£æ¬¾è¡¨ä¸­ï¼ˆéå…³é”®åˆ—ï¼‰: {sorted(list(repeated_non_key_fields))}ã€‚è¯·ä¿®æ”¹åˆ—åç¡®ä¿å”¯ä¸€æ€§ã€‚")

                         # G. æ‰§è¡Œ JSON è§„åˆ™æœ‰æ•ˆæ€§æ£€æŸ¥
                         available_fields = all_actual_source_fields.union(actual_deduction_fields)
                         invalid_source_map = []
                         invalid_target_map = []

                         # --- æ–°å¢ï¼šé¢„æ”¶é›†æ‰€æœ‰å®šä¹‰çš„ç›®æ ‡å­—æ®µ --- #
                         all_defined_target_fields = set()
                         for r in field_mappings:
                             for m in r.get("mappings", []):
                                 if m.get("target_field"):
                                     all_defined_target_fields.add(m["target_field"])
                         # --- ç»“æŸæ–°å¢ --- #

                         for rule_idx, rule in enumerate(field_mappings):
                             # --- ä½¿ç”¨ Session State --- #
                             rule_id = rule.get(st.session_state.single_selected_identity_column, f"è§„åˆ™ #{rule_idx}")
                             # --- ç»“æŸä½¿ç”¨ --- #
                             for map_idx, mapping in enumerate(rule.get("mappings", [])):
                                 if "source_field" in mapping: # ç®€å•æ˜ å°„
                                     src = mapping["source_field"]
                                     tgt = mapping.get("target_field")
                                     if src not in available_fields:
                                         invalid_source_map.append(f"è§„åˆ™ '{rule_id}': æºå­—æ®µ '{src}' åœ¨æºæ–‡ä»¶æˆ–æ‰£æ¬¾è¡¨ä¸­æœªæ‰¾åˆ°ã€‚")
                                     if template_available and tgt and tgt not in actual_template_fields:
                                         invalid_target_map.append(f"è§„åˆ™ '{rule_id}': ç›®æ ‡å­—æ®µ '{tgt}' (æ¥è‡ªæº '{src}') åœ¨æ¨¡æ¿æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ã€‚")
                                 elif "source_fields" in mapping: # å¤æ‚æ˜ å°„
                                     src_list = mapping["source_fields"]
                                     tgt = mapping.get("target_field")
                                     for src in src_list:
                                         # --- ä¿®æ”¹æ ¡éªŒæ¡ä»¶å’Œè­¦å‘Šæ¶ˆæ¯ --- #
                                         if src not in available_fields and src not in all_defined_target_fields:
                                             invalid_source_map.append(f"è§„åˆ™ '{rule_id}' (è®¡ç®—): æºå­—æ®µ '{src}' åœ¨æºæ–‡ä»¶/æ‰£æ¬¾è¡¨ä¸­æœªæ‰¾åˆ°ï¼Œä¸”æœªè¢«å…¶ä»–è§„åˆ™å®šä¹‰ä¸ºç›®æ ‡å­—æ®µã€‚")
                                         # --- ç»“æŸä¿®æ”¹ --- #
                                     if template_available and tgt and tgt not in actual_template_fields:
                                         invalid_target_map.append(f"è§„åˆ™ '{rule_id}' (è®¡ç®—): ç›®æ ‡å­—æ®µ '{tgt}' åœ¨æ¨¡æ¿æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ã€‚")

                         if invalid_source_map:
                              # --- ä¿®æ”¹è­¦å‘Šæ¶ˆæ¯æ ‡é¢˜å’Œæ ¼å¼ (ä½¿ç”¨ f-string) --- #
                              warning_list_md = "\n* ".join(invalid_source_map)
                              validation_warnings.append(f"**JSON è§„åˆ™è­¦å‘Šï¼šéƒ¨åˆ†è®¡ç®—æ‰€éœ€çš„æºå­—æ®µæ— æ³•ç›´æ¥ä»æ–‡ä»¶æˆ–ä»å…¶ä»–è§„åˆ™ç”Ÿæˆ (è¯·æ£€æŸ¥ JSON æˆ–æ–‡ä»¶):**\n* {warning_list_md}")
                              # --- ç»“æŸä¿®æ”¹ --- #
                         if invalid_target_map:
                              # --- ä¿®æ”¹é”™è¯¯æ¶ˆæ¯æ ‡é¢˜å’Œæ ¼å¼ (ä½¿ç”¨ f-string) --- #
                              error_list_md = "\n* ".join(invalid_target_map)
                              validation_errors.append(f"**JSON è§„åˆ™é”™è¯¯ï¼šéƒ¨åˆ†ç›®æ ‡å­—æ®µåœ¨æ¨¡æ¿æ–‡ä»¶ä¸­æœªæ‰¾åˆ°:**\n* {error_list_md}")
                              # --- ç»“æŸä¿®æ”¹ --- #

            except Exception as validation_ex:
                 validation_errors.append(f"æ ¡éªŒè¿‡ç¨‹ä¸­å‘ç”Ÿæ„å¤–é”™è¯¯: {validation_ex}")

            # H. æ˜¾ç¤ºæœ€ç»ˆæ ¡éªŒç»“æœ (å®Œæ•´ç‰ˆ)
            if validation_errors:
                 st.error("**æ ¡éªŒå¤±è´¥:**\n" + "\n".join(validation_errors))
                 st.session_state.validation_passed = False # å¯é€‰ï¼šç”¨äºåç»­æ§åˆ¶
            if validation_warnings:
                 st.warning("**æ ¡éªŒè­¦å‘Š:**\n" + "\n".join(validation_warnings))
            # åªæœ‰æ²¡æœ‰é”™è¯¯æ—¶æ‰ç®—é€šè¿‡ (å…è®¸æœ‰è­¦å‘Š)
            if not validation_errors and validation_warnings:
                 st.success("âœ… æ•°æ®å’Œè§„åˆ™æœ‰æ•ˆæ€§æ£€æŸ¥é€šè¿‡ (ä½†å­˜åœ¨è­¦å‘Š)ã€‚")
                 st.session_state.validation_passed = True
            elif not validation_errors and not validation_warnings:
                 st.success("âœ… æ•°æ®å’Œè§„åˆ™æœ‰æ•ˆæ€§æ£€æŸ¥é€šè¿‡ï¼")
                 st.session_state.validation_passed = True
            elif validation_errors: # ç¡®ä¿å¤±è´¥æ—¶çŠ¶æ€è¢«è®¾ç½®
                 if 'validation_passed' not in st.session_state or st.session_state.validation_passed is not False:
                     st.session_state.validation_passed = False

        # --- ç»“æŸå®ç°æ··åˆæ ¡éªŒé€»è¾‘ --- #

    validation_placeholder = st.container()
# --- ç»“æŸæ ¡éªŒåŒºåŸŸ --- #

# åªæœ‰å½“æ˜ å°„æ–‡ä»¶æœ‰æ•ˆæ—¶æ‰æ˜¾ç¤ºç¼–è¾‘å’Œå¯è§†åŒ–åŒºåŸŸ
if st.session_state.get('mapping_valid') is True:
    with st.expander("ğŸ“‹ æ˜ å°„è§„åˆ™ç¼–è¾‘ä¸å¯è§†åŒ–ï¼ˆæ”¯æŒæ–°å¢/æ ¡éªŒ/ä¸‹è½½ï¼‰", expanded=False):
        # å¯è§†åŒ–å­—æ®µæ˜ å°„ç»Ÿè®¡
        field_count = {"æœ‰æ•ˆæ˜ å°„": 0, "æºå­—æ®µç¼ºå¤±": 0, "ç›®æ ‡å­—æ®µç¼ºå¤±": 0} # Keep structure for now
        missing_target_details = [] # ç”¨äºå­˜å‚¨ç¼ºå¤±ç›®æ ‡å­—æ®µçš„è¯¦ç»†ä¿¡æ¯

        # ç¡®ä¿ current_sample_source_fields åœ¨è¿™é‡Œå¯ç”¨
        current_sample_source_fields = sample_source_fields

        for rule in field_mappings:
            # è·å–å½“å‰è§„åˆ™çš„æ ‡è¯†ç¬¦ï¼Œç”¨äºæ—¥å¿—/é”™è¯¯ä¿¡æ¯
            # ä½¿ç”¨ç”¨æˆ·åœ¨ selectbox ä¸­é€‰æ‹©çš„å­—æ®µå
            # --- ä½¿ç”¨ Session State --- #
            rule_identity_value = rule.get(st.session_state.single_selected_identity_column, "æœªçŸ¥è§„åˆ™æ ‡è¯†")
            # --- ç»“æŸä½¿ç”¨ --- #

            for mapping in rule.get("mappings", []):
                src = mapping.get("source_field", "")
                tgt = mapping.get("target_field", "")

                # åªå¯¹ç®€å•æ˜ å°„è¿›è¡Œæº/ç›®æ ‡å­—æ®µæ ¡éªŒ
                if "source_field" in mapping:
                    is_valid = False # æ ‡è®°å½“å‰æ˜ å°„æ˜¯å¦æœ‰æ•ˆ
                    # å¦‚æœæºå­—æ®µå­˜åœ¨ (åŸºäºä¸Šé¢çš„ç§»é™¤ï¼Œç°åœ¨æ€»æ˜¯æ‰§è¡Œè¿™ä¸ª else é€»è¾‘ï¼Œæ‰€ä»¥ç§»é™¤ else)
                    # æ£€æŸ¥ç›®æ ‡å­—æ®µæ˜¯å¦åœ¨æ¨¡æ¿ä¸­ (ä»…å½“ template_fields éç©ºæ—¶)
                    if template_fields and (tgt not in template_fields):
                        field_count["ç›®æ ‡å­—æ®µç¼ºå¤±"] += 1
                        missing_target_details.append({
                            "rule_id": rule_identity_value,
                            "source": src,
                            "target": tgt
                        })
                    else:
                         # æºå­—æ®µå­˜åœ¨ï¼Œä¸”(æ— æ¨¡æ¿ æˆ– ç›®æ ‡å­—æ®µåœ¨æ¨¡æ¿ä¸­)
                         field_count["æœ‰æ•ˆæ˜ å°„"] += 1
                         is_valid = True # is_valid ä¼¼ä¹æ²¡åœ¨åˆ«å¤„ç”¨ï¼Œä½†æš‚æ—¶ä¿ç•™
                    # (æ³¨æ„: å¤æ‚æ˜ å°„ ("source_fields") ä¸åœ¨æ­¤å¤„æ ¡éªŒæº/ç›®æ ‡å­—æ®µæ˜¯å¦å­˜åœ¨)

        # --- æ˜¾ç¤ºç¼ºå¤±å­—æ®µè¯¦æƒ… --- #
        st.markdown("---") # åˆ†éš”çº¿, ç§»é™¤ impunity
        if missing_target_details:
            st.warning("**ç›®æ ‡å­—æ®µç¼ºå¤±è¯¦æƒ… (ä¸æ¨¡æ¿æ–‡ä»¶å¯¹æ¯”):**")
            for detail in missing_target_details:
                st.markdown(f"- è§„åˆ™ **'{detail['rule_id']}'**: ç›®æ ‡å­—æ®µ `'{detail['target']}'` (æ¥è‡ª `'{detail['source']}'`) åœ¨æ¨¡æ¿æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ã€‚")
        # --- ç»“æŸæ–°å¢è¡¨å• ---

        # æ˜ å°„ç¼–è¾‘
        for i, rule in enumerate(field_mappings):
            # --- ä¿®æ”¹ï¼šä½¿ç”¨ Session State --- #
            identity_value = rule.get(st.session_state.single_selected_identity_column, 'æœªçŸ¥') # è·å–èº«ä»½å€¼
            bianzhi_value = rule.get('ç¼–åˆ¶', '') # ä¿ç•™ç¼–åˆ¶
            st.markdown(f"**{st.session_state.single_selected_identity_column}: {identity_value} | ç¼–åˆ¶: {bianzhi_value}**") # åŠ¨æ€æ˜¾ç¤º
            # --- ç»“æŸä¿®æ”¹ --- #
            for j, mapping in enumerate(rule.get("mappings", [])):
                if "source_field" in mapping:
                    col1, col2 = st.columns(2)
                    with col1:
                        mapping["source_field"] = st.text_input(f"æºå­—æ®µ #{i}-{j}", mapping.get("source_field", ""), key=f"src_{i}_{j}")
                    with col2:
                        mapping["target_field"] = st.text_input(f"ç›®æ ‡å­—æ®µ #{i}-{j}", mapping.get("target_field", ""), key=f"tgt_{i}_{j}_simple")
                elif "source_fields" in mapping:
                    col1, col2 = st.columns(2)
                    with col1:
                        st.markdown(f"**è®¡ç®—è§„åˆ™ #{i}-{j}**")
                        st.caption(f"æºå­—æ®µç»„: `{', '.join(mapping.get('source_fields', []))}`")
                        st.caption(f"è®¡ç®—æ–¹å¼: `{mapping.get('calculation', '')}`")
                    with col2:
                         mapping["target_field"] = st.text_input(f"ç›®æ ‡å­—æ®µ #{i}-{j}", mapping.get("target_field", ""), key=f"tgt_{i}_{j}_complex")
                else:
                    st.warning(f"è§„åˆ™ {i}-{j} æ ¼å¼æ— æ³•è¯†åˆ«: {mapping}")

        # ä¸‹è½½ä¿å­˜
        edited_json_for_download = json.dumps({"field_mappings": field_mappings}, ensure_ascii=False, indent=2)
        st.download_button("ğŸ“¥ ä¸‹è½½å½“å‰æ˜ å°„è§„åˆ™ JSON", edited_json_for_download, file_name="å­—æ®µæ˜ å°„è§„åˆ™_ç¼–è¾‘ç‰ˆ.json", mime="application/json")

        # ç§»é™¤ impunity å¹¶æ·»åŠ åˆé€‚çš„æŒ‰é’®æ–‡æœ¬
        if st.button("ğŸ’¾ ä¿å­˜æ˜ å°„è§„åˆ™å¤‡ä»½åˆ°æœ¬åœ°"):
            save_path = "./æ˜ å°„è§„åˆ™_ä¿å­˜å¤‡ä»½.json"
            try:
                with open(save_path, "w", encoding="utf-8") as f:
                    f.write(edited_json_for_download)
                st.success(f"å·²ä¿å­˜åˆ°ï¼š{save_path}")
            except Exception as save_err:
                st.error(f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {save_err}")

# --- å¤„ç†è§¦å‘åŒºåŸŸ ---
st.markdown("---") # åˆ†éš”çº¿

# --- æ–°å¢ï¼šæ ¹æ®æ ¡éªŒçŠ¶æ€å†³å®šæŒ‰é’®æ˜¯å¦å¯ç”¨ --- #
validation_status = st.session_state.get('validation_passed', None) # None: æœªæ ¡éªŒ, False: å¤±è´¥, True: æˆåŠŸ
disable_processing_button = (validation_status is not True)
button_tooltip = "è¯·å…ˆç‚¹å‡»ä¸Šæ–¹çš„ 'æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§' æŒ‰é’®å¹¶é€šè¿‡æ ¡éªŒã€‚" if disable_processing_button else "å¼€å§‹åˆå¹¶å¤„ç†æ‰€æœ‰ä¸Šä¼ çš„æ–‡ä»¶ã€‚"
# --- ç»“æŸ --- #

if st.button("ğŸš€ å¼€å§‹å¤„ç†æ•°æ®", type="primary", disabled=disable_processing_button, help=button_tooltip):
    # æ¸…ç©ºæ—§æ—¥å¿—å¹¶è®°å½•å¼€å§‹
    st.session_state.log_messages = []
    log("å¼€å§‹å¤„ç†æµç¨‹...", "INFO")

    # 1. è¾“å…¥æ ¡éªŒ
    valid_inputs = True
    if not source_files:
        log("è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªæºæ•°æ®å·¥èµ„è¡¨ï¼", "ERROR")
        valid_inputs = False
    if not file_deductions:
        log("è¯·ä¸Šä¼ æ‰£æ¬¾é¡¹è¡¨ï¼", "ERROR")
        valid_inputs = False
    if not st.session_state.get('mapping_valid', False):
        log("å­—æ®µæ˜ å°„æ–‡ä»¶æ— æ•ˆæˆ–æœªä¸Šä¼ ï¼", "ERROR")
        valid_inputs = False
    if not selected_identity_list:
        log("è¯·é€‰æ‹©ç”¨äºåŒ¹é…è§„åˆ™çš„å­—æ®µåï¼", "ERROR")
        valid_inputs = False
    if not key_identifier_columns:
        log("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªå…³é”®æ ‡è¯†åˆ—åï¼", "ERROR")
        valid_inputs = False

    if valid_inputs:
        # 2. å‡†å¤‡æ•°æ®
        deduction_df = None
        current_field_mappings = st.session_state.get('mapping_data', {}).get('field_mappings', [])
        selected_deduction_fields = [] # åˆå§‹åŒ–

        try:
            log("è¯»å–æ‰£æ¬¾æ•°æ®...", "INFO")
            with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp_deduct:
                file_deductions.seek(0)
                tmp_deduct.write(file_deductions.getvalue())
                tmp_deduct_path = tmp_deduct.name
            try:
                # è¯»å–æ‰£æ¬¾è¡¨ï¼Œä»ç¬¬ä¸‰è¡Œè¯»å–è¡¨å¤´
                deduction_df = pd.read_excel(tmp_deduct_path, header=2)
                # è®°å½•è¯»å–åˆ°çš„åˆ—åå’Œå‰å‡ è¡Œæ•°æ®
                log(f"è¯»å–åˆ°çš„æ‰£æ¬¾è¡¨åˆ—å: {deduction_df.columns.tolist()}", "INFO")
                log(f"æ‰£æ¬¾è¡¨æ˜ç»† (å‰ 5 è¡Œ): \n{deduction_df.head().to_string()}", "INFO")
            finally:
                os.unlink(tmp_deduct_path)

            # æ ¡éªŒæ‰£æ¬¾è¡¨å§“ååˆ—
            key_col_found = False
            name_columns = key_identifier_columns
            actual_name_col = None
            for key_col in name_columns:
                if key_col in deduction_df.columns:
                    key_col_found = True
                    actual_name_col = key_col # è®°å½•æ‰¾åˆ°çš„å§“ååˆ—
                    break
            if not key_col_found:
                log(f"æ‰£æ¬¾è¡¨å¿…é¡»åŒ…å«ç”¨æˆ·é€‰æ‹©çš„å…³é”®æ ‡è¯†åˆ—ä¸­çš„è‡³å°‘ä¸€ä¸ª ({name_columns})ï¼", "ERROR")
                st.stop()
            else:
                log(f"æ‰£æ¬¾æ•°æ®è¯»å–æˆåŠŸï¼Œæ‰¾åˆ°å…³é”®æ ‡è¯†åˆ—: '{actual_name_col}'ã€‚", "INFO")

            # è‡ªåŠ¨ç¡®å®šæ‰£æ¬¾å­—æ®µåˆ—è¡¨
            all_deduction_cols = deduction_df.columns.tolist()
            selected_deduction_fields = [col for col in all_deduction_cols if col != actual_name_col]
            log(f"è‡ªåŠ¨è¯†åˆ«ç”¨äºåˆå¹¶çš„æ‰£æ¬¾å­—æ®µ (å…± {len(selected_deduction_fields)} ä¸ª): {selected_deduction_fields}", "INFO")
            if not selected_deduction_fields:
                 log("è­¦å‘Šï¼šæ‰£æ¬¾è¡¨ä¸­é™¤äº†å§“ååˆ—å¤–æœªæ‰¾åˆ°å…¶ä»–å­—æ®µã€‚", "WARNING")

            # åœ¨è°ƒç”¨process_sheetä¹‹å‰æ·»åŠ é¢„å¤„ç†æ­¥éª¤
            print("\n=== é¢„å¤„ç†æ‰£æ¬¾æ•°æ® ===")
            print(f"æ‰£æ¬¾æ•°æ®å½¢çŠ¶: {deduction_df.shape}")
            print(f"æ‰£æ¬¾æ•°æ®åˆ—: {deduction_df.columns.tolist()}")
            print(f"æ‰£æ¬¾æ•°æ®å‰5è¡Œ:\n{deduction_df.head().to_string()}")

            # ç¡®ä¿æ‰€æœ‰é€‰ä¸­çš„æ‰£æ¬¾å­—æ®µéƒ½æ˜¯æ•°å€¼ç±»å‹
            for field in selected_deduction_fields:
                if field in deduction_df.columns:
                    deduction_df[field] = pd.to_numeric(deduction_df[field], errors='coerce').fillna(0)
                    print(f"\nå¤„ç†å­—æ®µ {field}:")
                    print(f"æ•°æ®ç±»å‹: {deduction_df[field].dtype}")
                    print(f"éé›¶å€¼æ•°é‡: {(deduction_df[field] != 0).sum()}")
                    print(f"å‰5ä¸ªå€¼: {deduction_df[field].head().to_list()}")

            # --- æ·»åŠ æ—¥å¿—ï¼šæ£€æŸ¥ 'è¡¥å‘å·¥èµ„' è§„åˆ™åŠ è½½æƒ…å†µ ---
            bufa_rule_found = False
            # --- æ–°å¢ï¼šåœ¨æ£€æŸ¥å‰å…ˆè·å– current_field_mappings --- #
            current_field_mappings = st.session_state.get('mapping_data', {}).get('field_mappings', [])
            # --- ç»“æŸæ–°å¢ --- #
            for identity_rule in field_mappings:
                for mapping in identity_rule.get("mappings", []):
                    if mapping.get("target_field") == "è¡¥å‘å·¥èµ„":
                        # --- ä½¿ç”¨ Session State --- #
                        log(f"DEBUG: Found rule for 'è¡¥å‘å·¥èµ„' under identity '{identity_rule.get(st.session_state.single_selected_identity_column, 'N/A')}':", "DEBUG")
                        # --- ç»“æŸä½¿ç”¨ --- #
                        log(f"  Source Fields: {mapping.get('source_fields')}", "DEBUG")
                        log(f"  Calculation: {mapping.get('calculation')}", "DEBUG")
                        bufa_rule_found = True
                        # break # å¯ä»¥å–æ¶ˆæ³¨é‡Šï¼Œå¦‚æœç¡®å®šæ¯ä¸ªèº«ä»½è§„åˆ™ä¸‹åªæœ‰ä¸€ä¸ªç›®æ ‡å­—æ®µ
                # if bufa_rule_found:
                    # break # å¯ä»¥å–æ¶ˆæ³¨é‡Šï¼Œå¦‚æœåªéœ€è¦æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…çš„è§„åˆ™
            if not bufa_rule_found:
                log("WARNING: No mapping rule found with target_field='è¡¥å‘å·¥èµ„' in loaded configuration.", "WARNING")
            # --- ç»“æŸæ·»åŠ  ---

            # --- æ–°å¢ï¼šé¢„è¿‡æ»¤æ˜ å°„è§„åˆ™ --- #
            log("å¼€å§‹é¢„è¿‡æ»¤æ˜ å°„è§„åˆ™...", "INFO")
            filtered_mappings_for_processing = []
            actual_deduction_fields = set(deduction_df.columns)
            # ä½¿ç”¨ä¹‹å‰è·å–çš„ sample_source_fields
            # æ³¨æ„ï¼šsample_source_fields å¯èƒ½æœªåœ¨æ‰€æœ‰åˆ†æ”¯åˆå§‹åŒ–ï¼Œéœ€è¦ç¡®ä¿å®ƒå­˜åœ¨
            if 'sample_source_fields' not in locals():
                 # å¦‚æœ sample_source_fields å› æŸç§åŸå› æœªå®šä¹‰ (ä¾‹å¦‚æ²¡æœ‰ä¸Šä¼ æºæ–‡ä»¶ï¼Œè™½ç„¶å‰é¢æœ‰æ ¡éªŒ)
                 # è¿™é‡Œå¯ä»¥è®¾ç½®ä¸ºç©ºé›†åˆï¼Œæˆ–è€…è®°å½•ä¸€ä¸ªé”™è¯¯ç„¶ååœæ­¢ï¼Ÿè®¾ç½®ä¸ºé›†åˆå¯èƒ½æ›´å®‰å…¨
                 log("è­¦å‘Šï¼šæœªèƒ½è·å–æºæ–‡ä»¶æ ·æœ¬å­—æ®µç”¨äºè§„åˆ™è¿‡æ»¤ï¼Œå°†ä¸æ‰§è¡Œè¿‡æ»¤ã€‚", "WARNING")
                 filtered_mappings_for_processing = current_field_mappings # ä¸è¿‡æ»¤
            else:
                filtered_rule_count = 0
                original_mapping_count = 0
                for rule in current_field_mappings:
                    original_mapping_count += len(rule.get("mappings", []))
                    filtered_rule = rule.copy()
                    filtered_rule["mappings"] = []
                    # --- ä½¿ç”¨ Session State --- #
                    rule_id_for_log = rule.get(st.session_state.single_selected_identity_column, 'æœªçŸ¥è§„åˆ™') # ç”¨äºæ—¥å¿—
                    # --- ç»“æŸä½¿ç”¨ --- #

                    for mapping in rule.get("mappings", []):
                        if "source_field" in mapping:
                            src = mapping["source_field"]
                            # æ¡ä»¶ï¼šæºå­—æ®µåœ¨æ‰£æ¬¾è¡¨å­˜åœ¨ ä¸” åœ¨æºæ–‡ä»¶æ ·æœ¬ä¸­ä¸å­˜åœ¨
                            if src in actual_deduction_fields and src not in sample_source_fields:
                                log(f"  - è¿‡æ»¤æ‰è§„åˆ™ '{rule_id_for_log}' ä¸­çš„æ— æ•ˆæ˜ å°„: æº '{src}' ä»…å­˜åœ¨äºæ‰£æ¬¾è¡¨ã€‚", "DEBUG")
                                filtered_rule_count += 1
                                continue # è·³è¿‡è¿™ä¸ªæ˜ å°„
                            else:
                                filtered_rule["mappings"].append(mapping) # ä¿ç•™æœ‰æ•ˆæ˜ å°„
                        elif "source_fields" in mapping:
                            filtered_rule["mappings"].append(mapping) # ä¿ç•™å¤æ‚æ˜ å°„
                        else:
                            # å¦‚æœæ˜ å°„æ ¼å¼æœªçŸ¥ï¼Œä¹Ÿä¿ç•™ï¼Ÿæˆ–è€…è­¦å‘Šï¼Ÿæš‚æ—¶ä¿ç•™
                            filtered_rule["mappings"].append(mapping)

                    filtered_mappings_for_processing.append(filtered_rule)
                log(f"æ˜ å°„è§„åˆ™é¢„è¿‡æ»¤å®Œæˆã€‚å…±è¿‡æ»¤æ‰ {filtered_rule_count} ä¸ªæ— æ•ˆçš„ç®€å•æ˜ å°„ã€‚", "INFO")
            # --- ç»“æŸé¢„è¿‡æ»¤ --- #

            # 3. å¤„ç†æ¯ä¸ªæºæ–‡ä»¶
            all_results = []
            has_error = False
            # --- ä½¿ç”¨ Session State --- #
            identity_column_to_use = st.session_state.single_selected_identity_column
            log(f"å¼€å§‹é€ä¸ªå¤„ç† {len(source_files)} ä¸ªæºæ–‡ä»¶... (ä½¿ç”¨ '{identity_column_to_use}' å­—æ®µåŒ¹é…)", "INFO")
            # --- ç»“æŸä½¿ç”¨ --- #
            with st.spinner(f"æ­£åœ¨å¤„ç† {len(source_files)} ä¸ªæºæ–‡ä»¶..."):
                for i, uploaded_file in enumerate(source_files):
                    log(f"[{i+1}/{len(source_files)}] å¤„ç†æ–‡ä»¶: {uploaded_file.name}", "INFO")
                    tmp_source_path = None
                    try:
                        with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp_source:
                            tmp_source.write(uploaded_file.getvalue())
                            tmp_source_path = tmp_source.name

                        # --- BEGIN: Add logging for source data before processing ---
                        try:
                            # Attempt to read source file to log info (need to find header)
                            preview_df_for_header = pd.read_excel(tmp_source_path, header=None, nrows=20) # Read first 20 rows to find header
                            # --- ä¿®æ”¹ï¼šä½¿ç”¨ key_identifier_columns --- #
                            header_row_source = next((idx for idx, row in preview_df_for_header.iterrows() if any((key_col in str(cell)) for cell in row.astype(str) for key_col in key_identifier_columns)), None)
                            # --- ç»“æŸä¿®æ”¹ --- #

                            if header_row_source is not None:
                                df_source_preview = pd.read_excel(tmp_source_path, header=header_row_source)
                                log(f"  -> æºæ–‡ä»¶ [{uploaded_file.name}] è¯»å–æˆåŠŸ (ä½¿ç”¨ {key_identifier_columns} æ£€æµ‹åˆ°è¡¨å¤´è¡Œ: {header_row_source + 1})ï¼Œå‡†å¤‡é€å…¥ process_sheet...", "INFO") # ä¿®æ”¹æ—¥å¿—
                                log(f"     æºæ–‡ä»¶åˆ—å: {df_source_preview.columns.tolist()}", "INFO")
                                log(f"     æºæ–‡ä»¶æ•°æ® (å‰ 5 è¡Œ):\\n{df_source_preview.head().to_string()}", "INFO")
                            else:
                                log(f"  -> è­¦å‘Š: æœªèƒ½åœ¨æºæ–‡ä»¶ [{uploaded_file.name}] å‰ 20 è¡Œæ‰¾åˆ° {key_identifier_columns} ä¸­çš„ä»»ä½•ä¸€ä¸ªä½œä¸ºè¡¨å¤´ï¼Œæ— æ³•è®°å½•æºæ•°æ®è¯¦æƒ…ã€‚", "WARNING") # ä¿®æ”¹æ—¥å¿—
                                # Optionally, proceed without preview logging or stop? For now, just warn.
                        except Exception as read_err:
                             log(f"  -> è­¦å‘Š: å°è¯•è¯»å–æºæ–‡ä»¶ [{uploaded_file.name}] è¿›è¡Œæ—¥å¿—è®°å½•æ—¶å‡ºé”™: {read_err}", "WARNING")
                        # --- END: Add logging for source data before processing ---

                        # --- BEGIN: Add simulated merge for diagnostics ---
                        if 'df_source_preview' in locals() and header_row_source is not None: # Ensure preview was read
                            try:
                                # --- ä¿®æ”¹ï¼šä½¿ç”¨ key_identifier_columns å’Œ actual_name_col --- #
                                source_key_to_use = None
                                if actual_name_col in df_source_preview.columns: # ä¼˜å…ˆä½¿ç”¨æ‰£æ¬¾è¡¨æ‰¾åˆ°çš„é‚£ä¸ª
                                    source_key_to_use = actual_name_col
                                else: # å¦åˆ™æŸ¥æ‰¾ç¬¬ä¸€ä¸ªåœ¨æºè¡¨ä¸­çš„ç”¨æˆ·é€‰æ‹©çš„å…³é”®åˆ—
                                     source_key_to_use = next((col for col in key_identifier_columns if col in df_source_preview.columns), None)

                                if source_key_to_use and actual_name_col: # ç¡®ä¿ä¸¤è¾¹éƒ½æœ‰å¯ç”¨çš„é”®
                                    log(f"  -> æ‰§è¡Œæ¨¡æ‹Ÿåˆå¹¶ (æº: {uploaded_file.name}, æ‰£æ¬¾è¡¨) on: æº='{source_key_to_use}', æ‰£æ¬¾='{actual_name_col}'...", "INFO") # ä¿®æ”¹æ—¥å¿—
                                    simulated_merge = pd.merge(df_source_preview, deduction_df, left_on=source_key_to_use, right_on=actual_name_col, how='left', suffixes=('', '_æ‰£æ¬¾')) # ä½¿ç”¨ left_on/right_on
                                    log(f"     æ¨¡æ‹Ÿåˆå¹¶ç»“æœåˆ—å: {simulated_merge.columns.tolist()}", "INFO")
                                    log(f"     æ¨¡æ‹Ÿåˆå¹¶ç»“æœæ•°æ® (å‰ 5 è¡Œ):\\n{simulated_merge.head().to_string()}", "INFO")
                                else:
                                    log(f"  -> è­¦å‘Š: æ— æ³•æ‰§è¡Œæ¨¡æ‹Ÿåˆå¹¶ï¼Œæºæ–‡ä»¶({key_identifier_columns})æˆ–æ‰£æ¬¾è¡¨({actual_name_col})ç¼ºå°‘æœ‰æ•ˆçš„å…¬å…±æˆ–æŒ‡å®šå…³é”®åˆ—ã€‚", "WARNING") # ä¿®æ”¹æ—¥å¿—
                                # --- ç»“æŸä¿®æ”¹ --- #
                            except Exception as merge_err:
                                log(f"  -> é”™è¯¯: æ‰§è¡Œæ¨¡æ‹Ÿåˆå¹¶æ—¶å‡ºé”™: {merge_err}", "ERROR")
                        else:
                             log(f"  -> è·³è¿‡æ¨¡æ‹Ÿåˆå¹¶ï¼Œå› ä¸ºæœªèƒ½æˆåŠŸè¯»å–æºæ–‡ä»¶é¢„è§ˆã€‚", "INFO")
                        # --- END: Add simulated merge for diagnostics ---

                        # æ·»åŠ è°ƒç”¨ process_sheet çš„æ—¥å¿—
                        log(f"  -> è°ƒç”¨æ ¸å¿ƒå¤„ç†å‡½æ•° process_sheet...", "INFO")
                        result_df = process_sheet(
                            tmp_source_path,
                            deduction_df,
                            filtered_mappings_for_processing,
                            selected_deduction_fields,
                            # --- ä½¿ç”¨ Session State --- #
                            identity_column_to_use,
                            identity_column_to_use # NOTE: Passing identity key twice? Check process_sheet definition if intended.
                            # --- ç»“æŸä½¿ç”¨ --- #
                         )
                        log(f"  <- process_sheet è¿”å›ï¼Œç»“æœè¡Œæ•°: {len(result_df) if result_df is not None else 'None'}", "INFO")

                        # --- BEGIN: Add logging for result data after processing ---
                        if result_df is not None and not result_df.empty:
                             log(f"     process_sheet è¿”å› [{uploaded_file.name}] åˆ—å: {result_df.columns.tolist()}", "INFO")
                             log(f"     process_sheet è¿”å› [{uploaded_file.name}] æ•°æ® (å‰ 5 è¡Œ):\\n{result_df.head().to_string()}", "INFO")
                             all_results.append(result_df)
                             log(f"[{i+1}/{len(source_files)}] æ–‡ä»¶ {uploaded_file.name} å¤„ç†æˆåŠŸã€‚", "SUCCESS")
                        # --- END: Add logging for result data after processing ---
                        else:
                             # Keep original warning log if result is None or empty
                             log(f"[{i+1}/{len(source_files)}] æ–‡ä»¶ {uploaded_file.name} æœªè¿”å›æœ‰æ•ˆæ•°æ® (å¯èƒ½æ— åŒ¹é…è¡Œæˆ–å¤„ç†é”™è¯¯)ã€‚", "WARNING")

                    except ValueError as ve: # æ•è· process_sheet è¿”å›çš„ç‰¹å®šé”™è¯¯ï¼Ÿè¿˜æ˜¯å†…éƒ¨å¤„ç†ï¼Ÿ
                        # å‡è®¾ process_sheet å†…éƒ¨å·²æ‰“å°é”™è¯¯ï¼Œè¿™é‡Œåªè®°å½•å¤±è´¥
                        log(f"[{i+1}/{len(source_files)}] æ–‡ä»¶ {uploaded_file.name} å¤„ç†å¤±è´¥ã€‚", "ERROR")
                        has_error = True
                    except Exception as e:
                        log(f"[{i+1}/{len(source_files)}] å¤„ç†æ–‡ä»¶ {uploaded_file.name} æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", "ERROR")
                        has_error = True
                    finally:
                        if tmp_source_path and os.path.exists(tmp_source_path):
                           os.unlink(tmp_source_path)
                    if has_error:
                         log(f"å› å¤„ç†æ–‡ä»¶ {uploaded_file.name} æ—¶å‘ç”Ÿé”™è¯¯ï¼Œå¤„ç†ä¸­æ­¢ã€‚", "ERROR")
                         break # ä¿æŒä¸­æ­¢é€»è¾‘

            # 4. åˆå¹¶ä¸æ ¼å¼åŒ–
            if not has_error and all_results:
                log("æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼Œå¼€å§‹åˆå¹¶ {len(all_results)} ä¸ªç»“æœ...", "INFO")
                with st.spinner("åˆå¹¶ç»“æœå¹¶æ ¼å¼åŒ–è¾“å‡º..."):
                    tmp_processed_path = None
                    output_path = None
                    try:
                        combined_df = pd.concat(all_results, ignore_index=True)
                        log("ç»“æœåˆå¹¶å®Œæˆï¼Œæ€»è¡Œæ•°: {len(combined_df)}", "INFO")

                        if file_template and template_fields:
                            # æ¢å¤ä¸ºä¸¥æ ¼æŒ‰æ¨¡æ¿ reindexï¼Œä¸¢å¼ƒä¸åœ¨æ¨¡æ¿ä¸­çš„åˆ—
                            log(f"æ ¹æ®æ¨¡æ¿æ–‡ä»¶çš„ {len(template_fields)} ä¸ªå­—æ®µä¸¥æ ¼ç­›é€‰å’Œæ’åºè¾“å‡ºåˆ—...", "INFO")
                            combined_df = combined_df.reindex(columns=template_fields)
                            # æ³¨æ„ï¼šå¦‚æœ template_fields åŒ…å« combined_df ä¸­æ²¡æœ‰çš„åˆ—ï¼Œreindex ä¼šæ·»åŠ å®ƒä»¬å¹¶å¡«å…… NaN
                        else:
                             log("æœªæä¾›æ¨¡æ¿æ–‡ä»¶æˆ–è¯»å–å¤±è´¥ï¼ŒæŒ‰åŸå§‹å¤„ç†é¡ºåºè¾“å‡ºæ‰€æœ‰åˆ—ã€‚", "INFO")

                        log("ä¿å­˜å¤„ç†ç»“æœåˆ°ä¸´æ—¶æ–‡ä»¶...", "INFO")
                        with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp_processed:
                            combined_df.to_excel(tmp_processed.name, index=False)
                            tmp_processed_path = tmp_processed.name

                        output_filename = f"{unit_name}_{salary_date.strftime('%Y%m')}_å·¥èµ„å‘æ”¾è¡¨_å·²å¤„ç†.xlsx"
                        output_dir = tempfile.mkdtemp()
                        output_path = os.path.join(output_dir, output_filename)

                        log("å¼€å§‹æ ¼å¼åŒ–è¾“å‡ºæ–‡ä»¶...", "INFO")
                        format_excel_with_styles(tmp_processed_path, output_path, salary_date.year, salary_date.month)
                        log("æ–‡ä»¶æ ¼å¼åŒ–å®Œæˆã€‚", "SUCCESS")

                        # åœ¨åˆå¹¶æ“ä½œåæ·»åŠ æ—¥å¿—
                        log(f"åˆå¹¶åçš„DataFrameåˆ—å: {combined_df.columns.tolist()}", "INFO")
                        log(f"åˆå¹¶åçš„DataFrameæ•°æ® (å‰ 5 è¡Œ):\n{combined_df.head().to_string()}", "INFO")

                        # åœ¨æ‰£æ¬¾æ˜ç»†è®¡ç®—å‰æ·»åŠ æ—¥å¿—
                        log(f"ç”¨äºè®¡ç®—çš„æ‰£æ¬¾æ˜ç»†å­—æ®µ: {selected_deduction_fields}", "INFO")
                        log(f"æ‰£æ¬¾æ˜ç»†å­—æ®µçš„å€¼ (å‰ 5 è¡Œ):\n{combined_df[selected_deduction_fields].head().to_string()}", "INFO")

                        # åœ¨æ‰£æ¬¾æ˜ç»†è®¡ç®—åæ·»åŠ æ—¥å¿—
                        # --- ä¿®æ”¹ï¼šç§»é™¤ç¡¬ç¼–ç è®¿é—® --- #
                        total_deduction_col_name = 'æ‰£å‘åˆè®¡'
                        if total_deduction_col_name in combined_df.columns:
                            log(f"è®¡ç®—åçš„'{total_deduction_col_name}'å’Œå…¶ä»–æ‰£æ¬¾æ˜ç»†å­—æ®µçš„å€¼ (å‰ 5 è¡Œ):\n{combined_df[[total_deduction_col_name] + selected_deduction_fields].head().to_string()}", "INFO")
                        else:
                            log(f"è®¡ç®—åçš„æ‰£æ¬¾æ˜ç»†å­—æ®µçš„å€¼ (æœªæ‰¾åˆ°'{total_deduction_col_name}'åˆ—) (å‰ 5 è¡Œ):\n{combined_df[selected_deduction_fields].head().to_string()}", "INFO")
                        # --- ç»“æŸä¿®æ”¹ --- #

                        # åœ¨æœ€ç»ˆè¾“å‡ºå‰æ·»åŠ æ—¥å¿—
                        log(f"æœ€ç»ˆè¾“å‡ºçš„DataFrameåˆ—å: {combined_df.columns.tolist()}", "INFO")
                        log(f"æœ€ç»ˆè¾“å‡ºçš„DataFrameæ•°æ® (å‰ 5 è¡Œ):\n{combined_df.head().to_string()}", "INFO")

                        # éªŒè¯ç»“æœ
                        print("\n=== éªŒè¯å¤„ç†ç»“æœ ===")
                        print(f"ç»“æœæ•°æ®å½¢çŠ¶: {result_df.shape}")
                        print(f"ç»“æœæ•°æ®åˆ—: {result_df.columns.tolist()}")

                        # æ£€æŸ¥æ‰£æ¬¾å­—æ®µ
                        for field in selected_deduction_fields:
                            if field in result_df.columns:
                                print(f"\næ£€æŸ¥å­—æ®µ {field}:")
                                print(f"æ•°æ®ç±»å‹: {result_df[field].dtype}")
                                print(f"éé›¶å€¼æ•°é‡: {(result_df[field] != 0).sum()}")
                                print(f"å‰5ä¸ªå€¼: {result_df[field].head().to_list()}")
                            else:
                                print(f"\nè­¦å‘Š: å­—æ®µ {field} ä¸åœ¨ç»“æœæ•°æ®ä¸­")

                        # æ£€æŸ¥å§“ååˆ—çš„åŒ¹é…æƒ…å†µ
                        # --- ä¿®æ”¹ï¼šä½¿ç”¨ key_identifier_columns --- #
                        common_key_for_validation = None
                        for key_col in key_identifier_columns:
                            if key_col in result_df.columns and key_col in deduction_df.columns:
                                common_key_for_validation = key_col
                                break

                        if common_key_for_validation:
                            print(f"\n=== ä½¿ç”¨å…³é”®åˆ— '{common_key_for_validation}' æ£€æŸ¥å§“ååŒ¹é…æƒ…å†µ ===")
                            source_names = set(result_df[common_key_for_validation].dropna().unique())
                            deduction_names = set(deduction_df[common_key_for_validation].dropna().unique())
                            matched_names = source_names.intersection(deduction_names)
                            print(f"æºæ–‡ä»¶ä¸­çš„ '{common_key_for_validation}' æ•°é‡: {len(source_names)}")
                            print(f"æ‰£æ¬¾è¡¨ä¸­çš„ '{common_key_for_validation}' æ•°é‡: {len(deduction_names)}")
                            print(f"æˆåŠŸåŒ¹é…çš„ '{common_key_for_validation}' æ•°é‡: {len(matched_names)}")
                            if len(matched_names) < len(source_names):
                                print(f"è­¦å‘Š: éƒ¨åˆ†æºæ–‡ä»¶ä¸­çš„ '{common_key_for_validation}' æœªèƒ½åŒ¹é…åˆ°æ‰£æ¬¾æ•°æ®")
                                print("æœªåŒ¹é…ç¤ºä¾‹:")
                                unmatched = source_names - matched_names
                                print(list(unmatched)[:5])
                        else:
                            print(f"\nè­¦å‘Š: æœªèƒ½åœ¨ç»“æœè¡¨å’Œæ‰£æ¬¾è¡¨ä¸­æ‰¾åˆ°å…±åŒçš„å…³é”®æ ‡è¯†åˆ— ({key_identifier_columns}) ç”¨äºåŒ¹é…éªŒè¯ã€‚")
                        # --- ç»“æŸä¿®æ”¹ --- #

                        # 5. æä¾›ä¸‹è½½
                        # st.success(f"ğŸ‰ å¤„ç†å®Œæˆï¼...") # ç”± log æ›¿ä»£
                        log(f"å¤„ç†æˆåŠŸå®Œæˆï¼æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆï¼š{output_filename}", "SUCCESS")
                        with open(output_path, "rb") as fp:
                            st.download_button(
                                label="ğŸ“¥ ä¸‹è½½æœ€ç»ˆæŠ¥å‘Š",
                                data=fp,
                                file_name=output_filename,
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                key="download_report"
                            )

                    except Exception as e:
                        log(f"åˆå¹¶æˆ–æ ¼å¼åŒ– Excel æ–‡ä»¶æ—¶å‡ºé”™: {e}", "ERROR")
                        has_error = True # ç¡®ä¿æ ‡è®°é”™è¯¯
                    finally:
                        if tmp_processed_path and os.path.exists(tmp_processed_path):
                            os.unlink(tmp_processed_path)

            elif not all_results and not has_error:
                 log("æœªç”Ÿæˆä»»ä½•æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥æºæ–‡ä»¶å†…å®¹å’Œæ˜ å°„è§„åˆ™ã€‚", "WARNING")
            elif has_error:
                log("å¤„ç†å› å‘ç”Ÿé”™è¯¯è€Œä¸­æ­¢ã€‚", "ERROR")

        except Exception as e:
            log(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿæ— æ³•æ¢å¤çš„ä¸¥é‡é”™è¯¯: {e}", "ERROR")
            # ç¡®ä¿æ¸…ç†å¯èƒ½é—ç•™çš„ä¸´æ—¶æ–‡ä»¶
            if 'tmp_deduct_path' in locals() and os.path.exists(tmp_deduct_path):
                os.unlink(tmp_deduct_path)
            if 'tmp_source_path' in locals() and os.path.exists(tmp_source_path):
                os.unlink(tmp_source_path)
            if 'tmp_processed_path' in locals() and os.path.exists(tmp_processed_path):
                os.unlink(tmp_processed_path)

    else:
        log("è¾“å…¥æ ¡éªŒå¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šä¼ çš„æ–‡ä»¶å’Œé…ç½®ã€‚", "ERROR")

# å¯ä»¥æ·»åŠ é¡µè„šç­‰ä¿¡æ¯
st.markdown("---")
st.caption("Â© æˆéƒ½é«˜æ–°åŒºè´¢æ”¿é‡‘èå±€")
